#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
äº¤æ˜“æµæ°´æ‰¹é‡åˆ†æå·¥å…· GUI   v6-plus (refactor + çº¿ä¸‹é“¶è¡Œæ‰©å±• + é€šä¿¡è”åŠ¨ä¿®å¤)
Author  : æ¸©å²­çºªå§”å…­å®¤ å•æŸ³æ˜Š   ï¼ˆ2025-08-05 ä¿®è®¢ï¼‰
é‡æ„è€…  : ï¼ˆæ•ˆç‡ä¼˜åŒ–ç‰ˆ 2025-08-28ï¼‰
æ‰©å±•è€…  : ï¼ˆçº¿ä¸‹å†œè¡Œ/å»ºè¡Œæ¥å…¥ 2025-09-09ï¼‰
è”åŠ¨è€…  : ï¼ˆé€šä¿¡è”åŠ¨ 2025-10-16ï¼Œå·ç â†’å§“å/èŒåŠ¡å›å†™é€šä¿¡ï¼Œå†ä»¥é€šä¿¡å§“åâ†’é“¶è¡Œâ€œå¯¹æ–¹èŒåŠ¡â€ï¼‰
ä¿®å¤è€…  : ï¼ˆé€šè®¯å½•åˆ—è¯†åˆ«&å·ç æ¸…æ´—ä¿®å¤ 2025-10-16 å¢å¼ºç‰ˆï¼‰

è¦ç‚¹ï¼š
- é€šè®¯å½•ï¼šæ‰‹æœºå·ä¼˜å…ˆè¯†åˆ«ï¼›è¡¨å¤´è‡ªåŠ¨æ¢æµ‹ï¼›â€œèŒåŠ¡â€= å®é™…å·¥ä½œå•ä½-ï¼ˆèŒåŠ¡ / èŒåŠ¡æˆ–å²—ä½ / å²—ä½ï¼‰ï¼›å·ç éç©ºå³å…¥åº“
- é€šä¿¡ï¼šä»¥ã€å¯¹æ–¹å·ç ã€‘ï¼ˆå€™é€‰åˆ—ï¼‰â†” é€šè®¯å½•ã€æ‰‹æœºå·ã€‘åŒ¹é… â†’ å›å†™ é€šä¿¡.å§“åã€é€šä¿¡.èŒåŠ¡
- é“¶è¡Œï¼šä»¥ é€šä¿¡.å§“å â†” é“¶è¡Œ.äº¤æ˜“å¯¹æ–¹å§“å åŒ¹é… â†’ å†™å…¥ é“¶è¡Œ.å¯¹æ–¹èŒåŠ¡
"""

import sys, os
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import threading, warnings, builtins, datetime, re
from pathlib import Path
from functools import lru_cache
from typing import Optional, List, Dict, Any, Tuple

import pandas as pd
import numpy as np
from chinese_calendar import is_holiday, is_workday
from decimal import Decimal, InvalidOperation

warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")

# ------------------------------------------------------------------
OUT_DIR: Optional[Path] = None
full_ts_pat = re.compile(r"\d{4}-\d{2}-\d{2}-\d{2}\.\d{2}\.\d{2}\.\d+")
TEMPLATE_COLS = [
    "åºå·","æŸ¥è¯¢å¯¹è±¡","åé¦ˆå•ä½","æŸ¥è¯¢é¡¹","æŸ¥è¯¢è´¦æˆ·","æŸ¥è¯¢å¡å·","äº¤æ˜“ç±»å‹","å€Ÿè´·æ ‡å¿—","å¸ç§",
    "äº¤æ˜“é‡‘é¢","è´¦æˆ·ä½™é¢","äº¤æ˜“æ—¶é—´","äº¤æ˜“æµæ°´å·","æœ¬æ–¹è´¦å·","æœ¬æ–¹å¡å·","äº¤æ˜“å¯¹æ–¹å§“å","äº¤æ˜“å¯¹æ–¹è´¦æˆ·",
    "äº¤æ˜“å¯¹æ–¹å¡å·","äº¤æ˜“å¯¹æ–¹è¯ä»¶å·ç ","äº¤æ˜“å¯¹æ‰‹ä½™é¢","äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ","äº¤æ˜“æ‘˜è¦","äº¤æ˜“ç½‘ç‚¹åç§°",
    "äº¤æ˜“ç½‘ç‚¹ä»£ç ","æ—¥å¿—å·","ä¼ ç¥¨å·","å‡­è¯ç§ç±»","å‡­è¯å·","ç°é‡‘æ ‡å¿—","ç»ˆç«¯å·","äº¤æ˜“æ˜¯å¦æˆåŠŸ",
    "äº¤æ˜“å‘ç”Ÿåœ°","å•†æˆ·åç§°","å•†æˆ·å·","IPåœ°å€","MAC","äº¤æ˜“æŸœå‘˜å·","å¤‡æ³¨",
]

# ===== å…¨å±€æ˜ å°„ =====
# é€šè®¯å½•ï¼šæ‰‹æœºå· -> (å§“å, èŒåŠ¡)   # èŒåŠ¡ = å®é™…å·¥ä½œå•ä½-ï¼ˆèŒåŠ¡/èŒåŠ¡æˆ–å²—ä½/å²—ä½ï¼‰
CONTACT_PHONE_TO_NAME_TITLE: Dict[str, Tuple[str, str]] = {}
# é€šä¿¡ï¼šå§“å -> èŒåŠ¡ï¼ˆä»…æ¥è‡ªå·ç åŒ¹é…æˆåŠŸçš„é€šä¿¡è®°å½•ï¼‰
CALLLOG_NAME_TO_TITLE: Dict[str, str] = {}

# ------------------------------------------------------------------
# åŸºç¡€å·¥å…·
# ------------------------------------------------------------------
SKIP_HEADER_KEYWORDS = ["åæ´—é’±-ç”µå­è´¦æˆ·äº¤æ˜“æ˜ç»†","ä¿¡ç”¨å¡æ¶ˆè´¹æ˜ç»†"]

@lru_cache(maxsize=None)
def _should_skip_special_cached(path_str: str) -> Optional[str]:
    p = Path(path_str)
    try:
        head = pd.read_excel(p, header=None, nrows=3)
        for kw in SKIP_HEADER_KEYWORDS:
            if head.astype(str).apply(lambda col: col.astype(str).str.contains(kw, na=False)).any().any():
                return kw
        return None
    except Exception:
        return None

def should_skip_special(p: Path) -> Optional[str]:
    return _should_skip_special_cached(str(p))

def _normalize_time(t: str, is_old: bool) -> str:
    if not t:
        return ""
    if "." in t:
        t = re.sub(r"\.(\d{1,6})$", lambda m: ":" + m.group(1).ljust(6, "0"), t.replace(".", ":"))
    if re.fullmatch(r"\d{6}", t):
        t = f"{t[:2]}:{t[2:4]}:{t[4:]}"
    elif re.fullmatch(r"\d{4}", t):
        t = f"{t[:2]}:{t[2:]}:00"
    if is_old and re.fullmatch(r"\d{6}", t.replace(":", "")):
        t_num = t.replace(":", "")
        t = f"{t_num[:2]}:{t_num[2:4]}:{t_num[4:]}"
    return t

def save_df_auto_width(
    df: pd.DataFrame,
    filename: Path | str,
    sheet_name: str = "Sheet1",
    index: bool = False,
    engine: str = "xlsxwriter",
    min_width: int = 6,
    max_width: int = 50,
):
    """
    â˜… ä¿®æ”¹ç‚¹ï¼ˆè¦†ç›–ä¿å­˜ï¼‰ï¼š
    - ä¿å­˜å‰è‹¥ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™å…ˆåˆ é™¤ï¼›å†ç”¨ mode='w' å†™å…¥ï¼Œç¡®ä¿è¦†ç›–ã€‚
    """
    if OUT_DIR is not None:
        filename = OUT_DIR / filename
    filename = Path(filename).with_suffix(".xlsx")
    filename.parent.mkdir(parents=True, exist_ok=True)

    # â˜… è¦†ç›–ä¿å­˜ï¼šå…ˆåˆ é™¤æ—§æ–‡ä»¶
    if filename.exists():
        try:
            filename.unlink()
        except Exception:
            pass

    df = df.replace(np.nan, "")
    if engine == "xlsxwriter":
        with pd.ExcelWriter(filename, engine="xlsxwriter", mode="w") as writer:  # â˜… æ˜ç¡®å†™å…¥æ¨¡å¼
            df.to_excel(writer, sheet_name=sheet_name, index=index)
            ws = writer.sheets[sheet_name]
            for i, col in enumerate(df.columns):
                s = df[col].astype(str)
                width = max(min_width, min(max(s.map(len).max(), len(str(col))) + 2, max_width))
                ws.set_column(i, i, width)
    else:
        # openpyxl æµç¨‹ä¹Ÿè¦†ç›–å†™
        with pd.ExcelWriter(filename, engine="openpyxl", mode="w") as writer:  # â˜… æ˜ç¡®å†™å…¥æ¨¡å¼
            df.to_excel(writer, sheet_name=sheet_name, index=index)
        from openpyxl import load_workbook
        wb = load_workbook(filename)
        ws = wb[sheet_name]
        for col_cells in ws.columns:
            width = max(len(str(c.value)) if c.value is not None else 0 for c in col_cells) + 2
            ws.column_dimensions[col_cells[0].column_letter].width = max(min_width, min(width, max_width)) + 5
        wb.save(filename)

def _iter_builtin_contacts_files() -> List[Path]:
    """
    è¿”å›å†…ç½®é€šè®¯å½•æ–‡ä»¶åˆ—è¡¨ï¼ˆä¼˜å…ˆè¯»åŒç›®å½•çš„ 'å†…ç½®-é€šè®¯å½•.xlsx'/'å†…ç½®-é€šè®¯å½•.xls'ï¼‰ã€‚
    åŒæ—¶å…¼å®¹ PyInstaller æ‰“åŒ…åçš„ä¸´æ—¶ç›®å½•ï¼ˆsys._MEIPASSï¼‰ã€‚
    """
    candidates = ["å†…ç½®-é€šè®¯å½•.xlsx", "å†…ç½®-é€šè®¯å½•.xls"]
    base_dirs: List[Path] = []

    # è¿è¡Œè„šæœ¬æ‰€åœ¨ç›®å½•
    try:
        base_dirs.append(Path(__file__).parent.resolve())
    except Exception:
        pass

    # PyInstaller æ‰“åŒ…åçš„ä¸´æ—¶å±•å¼€ç›®å½•
    if getattr(sys, "frozen", False) and hasattr(sys, "_MEIPASS"):
        base_dirs.append(Path(sys._MEIPASS).resolve())

    # å½“å‰å·¥ä½œç›®å½•ï¼ˆä»¥é˜²ä½ æŠŠå†…ç½®æ–‡ä»¶æ”¾åœ¨ cwdï¼‰
    try:
        base_dirs.append(Path.cwd().resolve())
    except Exception:
        pass

    out: List[Path] = []
    seen: set[str] = set()
    for b in base_dirs:
        for name in candidates:
            p = (b / name)
            try:
                rp = str(p.resolve())
            except Exception:
                rp = str(p)
            if p.exists() and rp not in seen:
                out.append(p)
                seen.add(rp)
    return out


# ------------------- å·ç æ¸…æ´—ï¼ˆå¢å¼ºç‰ˆï¼‰ -------------------
_MOBILE_PAT = re.compile(r'(?:\+?86[-\s]?)?(1[3-9]\d{9})')
def normalize_phone_cell(x: Any) -> str:
    """
    å¼ºåŒ–ç‰ˆæ¸…æ´—è§„åˆ™ï¼š
    - ä¼˜å…ˆæå–ä¸­å›½å¤§é™†æ‰‹æœºå·ï¼š1[3-9]å¼€å¤´11ä½ï¼ˆæ”¯æŒ+86/86- ç­‰å‰ç¼€ã€å„ç§åˆ†éš”ç¬¦ã€æ··å†™ï¼‰
    - è‹¥æœªå‘½ä¸­ï¼Œåˆ™ä¿ç•™æ•°å­—å¹¶å°è¯•ï¼š
        * ç§‘å­¦è®¡æ•°æ³•/å°æ•° .0
        * ä»…æ•°å­—æ—¶ç›´æ¥è¿”å›
    - è¿”å›ï¼šé¦–ä¸ªæ‰‹æœºå·ï¼›è‹¥æ²¡æœ‰æ‰‹æœºå·ä½†æœ‰å…¶ä»–æ•°å­—ï¼Œè¿”å›çº¯æ•°å­—ï¼›å®åœ¨æ²¡æœ‰è¿”å›ç©ºä¸²
    """
    if x is None or (isinstance(x, float) and not np.isfinite(x)):
        return ""
    if isinstance(x, (int, np.integer)):
        s = str(int(x))
        m = _MOBILE_PAT.search(s)
        return m.group(1) if m else s
    if isinstance(x, (float, np.floating)):
        try:
            s = str(int(x))
            m = _MOBILE_PAT.search(s)
            return m.group(1) if m else s
        except Exception:
            pass

    s = str(x).strip().replace("\u00A0", " ")
    if s == "":
        return ""

    m = _MOBILE_PAT.search(s)
    if m:
        return m.group(1)

    if re.fullmatch(r"\d+(\.0+)?", s):
        return s.split(".")[0]

    if re.fullmatch(r"[0-9]+(\.[0-9]+)?[eE][+-]?[0-9]+", s):
        try:
            d = Decimal(s)
            q = d.quantize(Decimal(1))
            ss = format(q, "f")
            m2 = _MOBILE_PAT.search(ss)
            return m2.group(1) if m2 else ss
        except InvalidOperation:
            pass

    only_digits = re.sub(r"\D", "", s)
    if len(only_digits) >= 11:
        m3 = _MOBILE_PAT.search(only_digits)
        if m3:
            return m3.group(1)
    return only_digits

def str_to_weekday(date_val) -> str:
    dt = pd.to_datetime(date_val, errors="coerce")
    return "wrong" if pd.isna(dt) else ["æ˜ŸæœŸä¸€","æ˜ŸæœŸäºŒ","æ˜ŸæœŸä¸‰","æ˜ŸæœŸå››","æ˜ŸæœŸäº”","æ˜ŸæœŸå…­","æ˜ŸæœŸæ—¥"][dt.weekday()]

def holiday_status(date_val) -> str:
    dt = pd.to_datetime(date_val, errors="coerce")
    if pd.isna(dt): return "wrong"
    d = dt.date()
    try:
        return "èŠ‚å‡æ—¥" if is_holiday(d) else ("å·¥ä½œæ—¥" if is_workday(d) else "å‘¨æœ«")
    except Exception:
        return "å‘¨æœ«" if dt.weekday() >= 5 else "å·¥ä½œæ—¥"

# ---------- CSV ----------
def _read_csv_smart(p: Path, **kwargs) -> pd.DataFrame:
    enc_try = ["utf-8-sig", "gb18030", "utf-8", "cp936"]
    last_err: Optional[Exception] = None
    for enc in enc_try:
        try:
            return pd.read_csv(p, encoding=enc, **kwargs)
        except Exception as e:
            last_err = e
    raise last_err or RuntimeError(f"æ— æ³•è¯»å–CSV: {p}")

def _person_from_people_csv(dirpath: Path) -> str:
    people = dirpath / "äººå‘˜ä¿¡æ¯.csv"
    if not people.exists():
        return ""
    try:
        df = _read_csv_smart(people)
    except Exception:
        return ""
    for col in ["å®¢æˆ·å§“å", "å§“å", "å®¢æˆ·åç§°", "æˆ·å"]:
        if col in df.columns:
            ser = df[col].astype(str).str.strip()
            ser = ser[(ser != "") & (ser.str.lower() != "nan")]
            if not ser.empty:
                return ser.iloc[0][:10]
    name_pat = re.compile(r"å®¢æˆ·(?:å§“å|åç§°)\s*[:ï¼š]?\s*([^\s:ï¼š]{2,10})")
    vals = df.astype(str).replace("nan", "", regex=False).to_numpy().ravel().tolist()
    for val in vals:
        m = name_pat.search(val.strip())
        if m: return m.group(1)
    return ""

# ------------------------------------------------------------------
# äººåè¾…åŠ©
# ------------------------------------------------------------------
NAME_CANDIDATE_COLS: List[str] = ["è´¦æˆ·åç§°","æˆ·å","è´¦æˆ·å","è´¦å·åç§°","è´¦å·å","å§“å","å®¢æˆ·åç§°","æŸ¥è¯¢å¯¹è±¡"]

def extract_holder_from_df(raw: pd.DataFrame) -> str:
    for col in raw.columns:
        if any(key in col for key in NAME_CANDIDATE_COLS):
            s = raw[col].dropna()
            if not s.empty:
                v = str(s.iloc[0]).strip()
                if v and len(v) <= 10:
                    return v
    return ""

def fallback_holder_from_path(p: Path) -> str:
    name = p.parent.name
    if "å†œå•†è¡Œ" in name:
        name = p.parent.parent.name if p.parent.parent != p.parent else ""
    if not name or "å†œå•†è¡Œ" in name:
        name = re.split(r"[-_]", p.stem)[0]
    return name or "æœªçŸ¥"

@lru_cache(maxsize=None)
def holder_from_folder(folder: Path) -> str:
    for fp in folder.glob("*.xls*"):
        try:
            header_idx = _header_row(fp)
            preview = pd.read_excel(fp, header=header_idx, nrows=5)
            if "è´¦æˆ·åç§°" in preview.columns:
                s = preview["è´¦æˆ·åç§°"].dropna()
                if not s.empty:
                    return str(s.iloc[0]).strip()
        except Exception:
            pass
    return ""

# ------------------------------------------------------------------
# è§£æå™¨ï¼ˆçœç•¥è¯´æ˜ï¼Œä¿æŒä½ åŸé€»è¾‘ä¸å˜ï¼‰
# ------------------------------------------------------------------
@lru_cache(maxsize=None)
def _header_row(path: Path) -> int:
    raw = pd.read_excel(path, header=None, nrows=15)
    for i, r in raw.iterrows():
        if "äº¤æ˜“æ—¥æœŸ" in r.values:
            return i
    return 0

def _read_raw(p: Path) -> pd.DataFrame:
    try:
        return pd.read_excel(p, header=_header_row(p))
    except Exception as e:
        print("âŒ", p.name, e)
        return pd.DataFrame()

def _parse_dt(d, t, is_old):
    try:
        if isinstance(t, str) and full_ts_pat.fullmatch(t.strip()):
            dt = pd.to_datetime(t, format="%Y-%m-%d-%H.%M.%S.%f", errors="coerce")
        else:
            dt = pd.to_datetime(f"{d} {_normalize_time(str(t), is_old)}".strip(), errors="coerce")
        return dt.strftime("%Y-%m-%d %H:%M:%S") if pd.notna(dt) else "wrong"
    except Exception:
        return "wrong"

# ------------------------------------------------------------------
# CSV â†’ æ¨¡æ¿ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
# ------------------------------------------------------------------
def csv_to_template(raw: pd.DataFrame, holder: str, feedback_unit: str) -> pd.DataFrame:
    if raw.empty:
        return pd.DataFrame(columns=TEMPLATE_COLS)
    try:
        df = raw.copy()
        df.columns = pd.Index(df.columns).astype(str).str.strip()
        n = len(df)
        def _S(default=""): return pd.Series([default]*n, index=df.index)
        def col(keys, default=""):
            if isinstance(keys, str): return df[keys] if keys in df else _S(default)
            for k in keys:
                if k in df: return df[k]
            return _S(default)
        def _to_str_no_sci(x):
            if pd.isna(x): return ""
            s = str(x).strip()
            if s.lower()=="nan": return ""
            if re.fullmatch(r"\d+\.0", s): return s[:-2]
            try:
                if isinstance(x, (int, np.integer)): return str(int(x))
                if isinstance(x, float): return f"{x:.0f}" if np.isfinite(x) else ""
                if re.fullmatch(r"\d+(\.\d+)?[eE][+-]?\d+", s): return f"{float(s):.0f}"
            except Exception: pass
            return s
        def _std_success(v):
            s = str(v).strip()
            if s in {"1","Y","y","æ˜¯","æˆåŠŸ","True","true"}: return "æˆåŠŸ"
            if s in {"0","N","n","å¦","å¤±è´¥","False","false"}: return "å¤±è´¥"
            return "" if s.lower()=="nan" else s
        out = pd.DataFrame(index=df.index)
        out["æœ¬æ–¹è´¦å·"] = col(["äº¤æ˜“è´¦å·","æŸ¥è¯¢è´¦æˆ·","æœ¬æ–¹è´¦å·","è´¦å·","è´¦å·/å¡å·","è´¦å·å¡å·"]).map(_to_str_no_sci)
        out["æœ¬æ–¹å¡å·"] = col(["äº¤æ˜“å¡å·","æŸ¥è¯¢å¡å·","æœ¬æ–¹å¡å·","å¡å·"]).map(_to_str_no_sci)
        out["æŸ¥è¯¢è´¦æˆ·"] = out["æœ¬æ–¹è´¦å·"]; out["æŸ¥è¯¢å¡å·"]=out["æœ¬æ–¹å¡å·"]
        opp_no  = col(["äº¤æ˜“å¯¹æ‰‹è´¦å¡å·","äº¤æ˜“å¯¹æ‰‹è´¦å·","å¯¹æ–¹è´¦å·","å¯¹æ–¹è´¦æˆ·"]).map(_to_str_no_sci)
        opp_typ = col(["äº¤æ˜“å¯¹æ–¹å¸å¡å·ç±»å‹","è´¦å·/å¡å·ç±»å‹"], "")
        typ_s   = opp_typ.astype(str)
        is_card = typ_s.str.contains("å¡", na=False) | typ_s.isin(["2","å¡","å¡å·"])
        out["äº¤æ˜“å¯¹æ–¹å¡å·"] = np.where(is_card, opp_no, ""); out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"]=np.where(is_card, "", opp_no)
        out["æŸ¥è¯¢å¯¹è±¡"] = holder or "æœªçŸ¥"; out["åé¦ˆå•ä½"]=feedback_unit or "æœªçŸ¥"
        out["å¸ç§"] = col(["äº¤æ˜“å¸ç§","å¸ç§","å¸åˆ«","è´§å¸"], "CNY").astype(str).replace(
            {"äººæ°‘å¸":"CNY","äººæ°‘å¸å…ƒ":"CNY","RMB":"CNY","156":"CNY"}).fillna("CNY")
        out["äº¤æ˜“é‡‘é¢"] = pd.to_numeric(col(["äº¤æ˜“é‡‘é¢","é‡‘é¢","å‘ç”Ÿé¢"], 0), errors="coerce")
        out["è´¦æˆ·ä½™é¢"] = pd.to_numeric(col(["äº¤æ˜“ä½™é¢","ä½™é¢","è´¦æˆ·ä½™é¢"], 0), errors="coerce")
        out["å€Ÿè´·æ ‡å¿—"] = col(["æ”¶ä»˜æ ‡å¿—",""], "")
        if "äº¤æ˜“æ—¶é—´" in df.columns:
            tt = pd.to_datetime(df["äº¤æ˜“æ—¶é—´"], errors="coerce")
            out["äº¤æ˜“æ—¶é—´"] = np.where(tt.notna(), tt.dt.strftime("%Y-%m-%d %H:%M:%S"), df["äº¤æ˜“æ—¶é—´"].astype(str))
        else:
            out["äº¤æ˜“æ—¶é—´"] = _S("wrong")
        out["äº¤æ˜“ç±»å‹"] = col(["äº¤æ˜“ç±»å‹","ä¸šåŠ¡ç§ç±»","äº¤æ˜“ç "], "")
        out["äº¤æ˜“æµæ°´å·"] = col(["äº¤æ˜“æµæ°´å·","æŸœå‘˜æµæ°´å·","æµæ°´å·"], "")
        out["äº¤æ˜“å¯¹æ–¹å§“å"] = col(["å¯¹æ‰‹æˆ·å","äº¤æ˜“å¯¹æ‰‹åç§°","å¯¹æ‰‹æ–¹åç§°","å¯¹æ–¹æˆ·å","å¯¹æ–¹åç§°","å¯¹æ–¹å§“å","æ”¶/ä»˜æ–¹åç§°"], " ")
        out["äº¤æ˜“å¯¹æ–¹è¯ä»¶å·ç "] = col(["å¯¹æ‰‹èº«ä»½è¯å·","å¯¹æ–¹è¯ä»¶å·ç "], " ")
        out["äº¤æ˜“å¯¹æ‰‹ä½™é¢"] = pd.to_numeric(col(["å¯¹æ‰‹äº¤æ˜“ä½™é¢"], ""), errors="coerce")
        out["äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ"] = col(["å¯¹æ‰‹å¼€æˆ·é“¶è¡Œ","äº¤æ˜“å¯¹æ‰‹è¡Œå","å¯¹æ–¹å¼€æˆ·è¡Œ","å¯¹æ–¹é‡‘èæœºæ„åç§°"], " ")
        out["äº¤æ˜“æ‘˜è¦"] = col(["æ‘˜è¦è¯´æ˜","äº¤æ˜“æ‘˜è¦","æ‘˜è¦","é™„è¨€","ç”¨é€”"], " ")
        out["äº¤æ˜“ç½‘ç‚¹åç§°"] = col(["äº¤æ˜“ç½‘ç‚¹åç§°","äº¤æ˜“æœºæ„","ç½‘ç‚¹åç§°"], "")
        out["äº¤æ˜“ç½‘ç‚¹ä»£ç "] = col(["äº¤æ˜“ç½‘ç‚¹ä»£ç ","æœºæ„å·","ç½‘ç‚¹ä»£ç "], "")
        out["æ—¥å¿—å·"] = col(["æ—¥å¿—å·"], ""); out["ä¼ ç¥¨å·"] = col(["ä¼ ç¥¨å·"], "")
        out["å‡­è¯ç§ç±»"] = col(["å‡­è¯ç§ç±»","å‡­è¯ç±»å‹"], ""); out["å‡­è¯å·"]=col(["å‡­è¯å·","å‡­è¯åºå·"], "")
        out["ç°é‡‘æ ‡å¿—"] = col(["ç°é‡‘æ ‡å¿—"], ""); out["ç»ˆç«¯å·"]=col(["ç»ˆç«¯å·","æ¸ é“å·"], "")
        succ = col(["äº¤æ˜“æ˜¯å¦æˆåŠŸ","æŸ¥è¯¢åé¦ˆç»“æœ"], ""); out["äº¤æ˜“æ˜¯å¦æˆåŠŸ"]=succ.map(_std_success)
        out["äº¤æ˜“å‘ç”Ÿåœ°"] = col(["äº¤æ˜“å‘ç”Ÿåœ°","äº¤æ˜“åœºæ‰€"], ""); out["å•†æˆ·åç§°"]=col(["å•†æˆ·åç§°"], ""); out["å•†æˆ·å·"]=col(["å•†æˆ·å·"], "")
        out["IPåœ°å€"]=col(["IPåœ°å€"], ""); out["MAC"]=col(["MACåœ°å€","MAC"], ""); out["äº¤æ˜“æŸœå‘˜å·"]=col(["äº¤æ˜“æŸœå‘˜å·","æŸœå‘˜å·","è®°è´¦æŸœå‘˜"], "")
        try:
            beizhu = col(["å¤‡æ³¨","é™„è¨€","è¯´æ˜"], "").astype(str); reason = col(["æŸ¥è¯¢åé¦ˆç»“æœåŸå› "], "").astype(str)
            beizhu_clean = beizhu.where(~beizhu.str.lower().eq("nan"), ""); reason_clean = reason.where(~reason.str.lower().eq("nan"), "")
            out["å¤‡æ³¨"] = np.where(reason_clean!="", np.where(beizhu_clean!="" , beizhu_clean+"ï½œåŸå› ï¼š"+reason_clean, "åŸå› ï¼š"+reason_clean), beizhu_clean)
        except Exception:
            out["å¤‡æ³¨"] = _S("wrong")
        return out.reindex(columns=TEMPLATE_COLS, fill_value="")
    except Exception as e:
        print(f"âŒ CSVè½¬æ¨¡æ¿å¼‚å¸¸ï¼š{e}")
        return pd.DataFrame(columns=TEMPLATE_COLS)

# ===============================
# å„é“¶è¡Œè§£æï¼ˆä¿æŒåŸå®ç°ï¼Œç•¥å»æ³¨é‡Šï¼‰
# ===============================
def tl_to_template(raw) -> pd.DataFrame:
    if isinstance(raw, dict):
        frames=[]
        for sheet_name, df_sheet in raw.items():
            one = tl_to_template(df_sheet)
            if not one.empty:
                one.insert(0,"__sheet__",sheet_name); frames.append(one)
        return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=TEMPLATE_COLS)
    df: pd.DataFrame = raw
    if df.empty: return pd.DataFrame(columns=TEMPLATE_COLS)
    def col_multi(keys, default=""):
        for k in keys:
            if k in df: return df[k]
        return pd.Series([default]*len(df), index=df.index)
    out = pd.DataFrame(index=df.index)
    out["æœ¬æ–¹è´¦å·"] = col_multi(["å®¢æˆ·è´¦å·","è´¦å·","æœ¬æ–¹è´¦å·"], "wrong")
    out["æŸ¥è¯¢è´¦æˆ·"] = out["æœ¬æ–¹è´¦å·"]; out["åé¦ˆå•ä½"]="æ³°éš†é“¶è¡Œ"
    out["æŸ¥è¯¢å¯¹è±¡"] = col_multi(["è´¦æˆ·åç§°","æˆ·å","å®¢æˆ·åç§°"], "wrong")
    out["å¸ç§"] = col_multi(["å¸ç§","è´§å¸","å¸åˆ«"]).replace("156","CNY").replace("äººæ°‘å¸å…ƒ","CNY").replace("äººæ°‘å¸","CNY").fillna("CNY")
    out["å€Ÿè´·æ ‡å¿—"] = col_multi(["å€Ÿè´·æ ‡å¿—","å€Ÿè´·æ–¹å‘","å€Ÿè´·"], "")
    debit  = pd.to_numeric(col_multi(["å€Ÿæ–¹å‘ç”Ÿé¢","å€Ÿæ–¹å‘ç”Ÿé‡‘é¢"], 0), errors="coerce")
    credit = pd.to_numeric(col_multi(["è´·æ–¹å‘ç”Ÿé¢","è´·æ–¹å‘ç”Ÿé‡‘é¢"], 0), errors="coerce")
    out["äº¤æ˜“é‡‘é¢"] = debit.fillna(0).where(debit.ne(0), credit)
    out["è´¦æˆ·ä½™é¢"] = pd.to_numeric(col_multi(["è´¦æˆ·ä½™é¢","ä½™é¢"], 0), errors="coerce")
    dates = col_multi(["äº¤æ˜“æ—¥æœŸ","åŸäº¤æ˜“æ—¥æœŸ","ä¼šè®¡æ—¥æœŸ"]).astype(str)
    raw_times = col_multi(["äº¤æ˜“æ—¶é—´","åŸäº¤æ˜“æ—¶é—´","æ—¶é—´"]).astype(str).str.strip()
    def _tidy_time(s: str) -> str:
        if re.fullmatch(r"0+(\.0+)?", s): return ""
        if s.count(".") >= 2:
            p = s.split(".")
            if len(p[0])==2 and len(p[1])==2 and len(p[2])==2: return ".".join(p[:3])
        return s
    def _clean_time(s: str) -> str:
        s=s.strip()
        if re.fullmatch(r"0+(\.0+)?", s): return ""
        if re.fullmatch(r"\d{1,9}", s): return s.zfill(9)[:6]
        return s
    times = raw_times.apply(lambda x:_clean_time(_tidy_time(x)))
    out["äº¤æ˜“æ—¶é—´"] = [ _parse_dt(d,t,False) for d,t in zip(dates,times)]
    out["äº¤æ˜“æµæ°´å·"] = col_multi(["åŸæŸœå‘˜æµæ°´å·","æµæ°´å·"])
    out["äº¤æ˜“ç±»å‹"] = col_multi(["äº¤æ˜“ç ","äº¤æ˜“ç±»å‹","ä¸šåŠ¡ç§ç±»"])
    out["äº¤æ˜“å¯¹æ–¹å§“å"] = col_multi(["å¯¹æ–¹æˆ·å","äº¤æ˜“å¯¹æ‰‹åç§°"], " ")
    out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"] = col_multi(["å¯¹æ–¹å®¢æˆ·è´¦å·","å¯¹æ–¹è´¦å·"], " ")
    out["äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ"] = col_multi(["å¯¹æ–¹é‡‘èæœºæ„åç§°","å¯¹æ–¹å¼€æˆ·è¡Œ"], " ")
    out["äº¤æ˜“æ‘˜è¦"] = col_multi(["æ‘˜è¦æè¿°","æ‘˜è¦"], " ")
    out["äº¤æ˜“ç½‘ç‚¹ä»£ç "] = col_multi(["æœºæ„å·","ç½‘ç‚¹ä»£ç "], " ")
    out["ç»ˆç«¯å·"] = col_multi(["æ¸ é“å·","ç»ˆç«¯å·"], " ")
    out["äº¤æ˜“æŸœå‘˜å·"] = col_multi(["æŸœå‘˜å·"], " ")
    out["å¤‡æ³¨"] = col_multi(["å¤‡æ³¨","é™„è¨€"], " ")
    out["å‡­è¯ç§ç±»"] = col_multi(["å‡­è¯ç±»å‹"], ""); out["å‡­è¯å·"]=col_multi(["å‡­è¯åºå·"], "")
    return out.reindex(columns=TEMPLATE_COLS, fill_value="")

def mt_to_template(raw: pd.DataFrame) -> pd.DataFrame:
    if raw.empty: return pd.DataFrame(columns=TEMPLATE_COLS)
    header_idx=None
    for i,row in raw.iterrows():
        cells=row.astype(str).str.strip().tolist()
        if "æ—¶é—´" in cells and "è´¦å·å¡å·" in cells:
            header_idx=i;break
    if header_idx is None:
        for i,row in raw.iterrows():
            if row.astype(str).str.contains("åºå·").any():
                header_idx=i;break
    if header_idx is None: return pd.DataFrame(columns=TEMPLATE_COLS)
    holder=""
    name_inline=re.compile(r"å®¢æˆ·(?:å§“å|åç§°)\s*[:ï¼š]?\s*([^\s:ï¼š]{2,10})")
    for i in range(header_idx):
        vals=raw.iloc[i].astype(str).tolist()
        for j,cell in enumerate(vals):
            cs=cell.strip(); m=name_inline.match(cs)
            if m: holder=m.group(1); break
            if re.fullmatch(r"å®¢æˆ·(?:å§“å|åç§°)\s*[:ï¼š]?", cs):
                nxt=str(vals[j+1]).strip() if j+1<len(vals) else ""
                if nxt and nxt.lower()!="nan": holder=nxt; break
        if holder: break
    holder=holder or "æœªçŸ¥"
    df=raw.iloc[header_idx+1:].copy(); df.columns=raw.iloc[header_idx].astype(str).str.strip()
    df.dropna(how="all", inplace=True); df.reset_index(drop=True, inplace=True)
    summary_mask = df.apply(lambda row: row.astype(str).str.contains(r"æ”¯å‡ºç¬”æ•°|æ”¶å…¥ç¬”æ•°|æ”¯å‡ºç´¯è®¡é‡‘é¢|æ”¶å…¥ç´¯è®¡é‡‘é¢").any(), axis=1)
    df=df[~summary_mask].copy()
    def col(c, default=""): return df[c] if c in df else pd.Series(default, index=df.index)
    out=pd.DataFrame(index=df.index)
    acct=col("è´¦å·å¡å·").astype(str).str.replace(r"\.0$","",regex=True)
    out["æœ¬æ–¹è´¦å·"]=acct; out["æŸ¥è¯¢è´¦æˆ·"]=acct; out["æŸ¥è¯¢å¯¹è±¡"]=holder; out["åé¦ˆå•ä½"]="æ°‘æ³°é“¶è¡Œ"
    out["å¸ç§"]=col("å¸ç§").astype(str).replace("äººæ°‘å¸","CNY").replace("äººæ°‘å¸å…ƒ","CNY").fillna("CNY")
    debit=pd.to_numeric(col("æ”¯å‡º"), errors="coerce").fillna(0)
    credit=pd.to_numeric(col("æ”¶å…¥"), errors="coerce").fillna(0)
    out["äº¤æ˜“é‡‘é¢"]=credit.where(credit.gt(0), -debit)
    out["è´¦æˆ·ä½™é¢"]=pd.to_numeric(col("ä½™é¢"), errors="coerce")
    out["å€Ÿè´·æ ‡å¿—"]=np.where(credit.gt(0),"è¿›","å‡º")
    def _fmt_time(v:str)->str:
        v=str(v).strip()
        try: return datetime.datetime.strptime(v,"%Y%m%d %H:%M:%S").strftime("%Y-%m-%d %H:%M:%S")
        except Exception: return v or "wrong"
    out["äº¤æ˜“æ—¶é—´"]=col("æ—¶é—´").astype(str).apply(_fmt_time)
    out["äº¤æ˜“æ‘˜è¦"]=col("æ‘˜è¦"," "); out["äº¤æ˜“æµæ°´å·"]=col("æŸœå‘˜æµæ°´å·").astype(str).str.strip()
    out["äº¤æ˜“æŸœå‘˜å·"]=col("è®°è´¦æŸœå‘˜ ").astype(str).str.strip(); out["äº¤æ˜“ç½‘ç‚¹ä»£ç "]=col("è®°è´¦æœºæ„").astype(str).str.strip()
    out["äº¤æ˜“å¯¹æ–¹å§“å"]=col("äº¤æ˜“å¯¹æ‰‹åç§°"," ").astype(str).str.strip()
    out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"]=col("äº¤æ˜“å¯¹æ‰‹è´¦å·"," ").astype(str).str.strip()
    out["äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ"]=col("äº¤æ˜“å¯¹æ‰‹è¡Œå"," ").astype(str).str.strip()
    out["ç»ˆç«¯å·"]=col("äº¤æ˜“æ¸ é“"); out["å¤‡æ³¨"]=col("é™„è¨€"," ")
    return out.reindex(columns=TEMPLATE_COLS, fill_value="")

def rc_to_template(raw: pd.DataFrame, holder: str, is_old: bool) -> pd.DataFrame:
    if raw.empty: return pd.DataFrame(columns=TEMPLATE_COLS)
    def col(c, default=""): return raw[c] if c in raw else pd.Series([default]*len(raw), index=raw.index)
    out=pd.DataFrame(index=raw.index)
    out["æœ¬æ–¹è´¦å·"]=col("è´¦å·","wrong"); out["æŸ¥è¯¢è´¦æˆ·"]=out["æœ¬æ–¹è´¦å·"]
    out["äº¤æ˜“é‡‘é¢"]=col("å‘ç”Ÿé¢") if is_old else col("äº¤æ˜“é‡‘é¢")
    out["è´¦æˆ·ä½™é¢"]=col("ä½™é¢") if is_old else col("äº¤æ˜“ä½™é¢")
    out["åé¦ˆå•ä½"]="è€å†œå•†é“¶è¡Œ" if is_old else "æ–°å†œå•†é“¶è¡Œ"
    dates=col("äº¤æ˜“æ—¥æœŸ").astype(str); times=col("äº¤æ˜“æ—¶é—´").astype(str)
    out["äº¤æ˜“æ—¶é—´"]=[_parse_dt(d,t,is_old) for d,t in zip(dates,times)]
    out["å€Ÿè´·æ ‡å¿—"]=col("å€Ÿè´·æ ‡å¿—")
    out["å¸ç§"]="CNY" if is_old else col("å¸ç§").replace("äººæ°‘å¸","CNY").replace("äººæ°‘å¸å…ƒ","CNY")
    out["æŸ¥è¯¢å¯¹è±¡"]=holder
    out["äº¤æ˜“å¯¹æ–¹å§“å"]=col("å¯¹æ–¹å§“å"," "); out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"]=col("å¯¹æ–¹è´¦å·"," ")
    out["äº¤æ˜“ç½‘ç‚¹åç§°"]=col("ä»£ç†è¡Œæœºæ„å·") if is_old else col("äº¤æ˜“æœºæ„")
    out["äº¤æ˜“æ‘˜è¦"]=col("å¤‡æ³¨") if is_old else col("æ‘˜è¦","wrong")
    return out.reindex(columns=TEMPLATE_COLS, fill_value="")

# ---- å†œè¡Œçº¿ä¸‹ APSH ----
def _is_abc_offline_file(p: Path) -> bool:
    try: xls = pd.ExcelFile(p); return "APSH" in xls.sheet_names
    except Exception: return False

def _merge_abc_datetime(date_val, time_val) -> str:
    ds = re.sub(r"\D","", "" if date_val is None else str(date_val).strip())
    date_ts = pd.to_datetime(ds[:8], format="%Y%m%d", errors="coerce") if len(ds)>=8 else pd.to_datetime(date_val, errors="coerce")
    if pd.isna(date_ts): return "wrong"
    def to_hhmmss(t)->str:
        if t is None or (isinstance(t,float) and np.isnan(t)) or t=="" or pd.isna(t): return "000000"
        if isinstance(t,(int,np.integer,float,np.floating)):
            try:
                tf=float(t)
                if 0<=tf<1:
                    secs=int(round(tf*86400)); secs=0 if secs>=86400 else secs
                    h=secs//3600; m=(secs%3600)//60; s=secs%60
                    return f"{h:02d}{m:02d}{s:02d}"
                digits=re.sub(r"\D","",str(int(round(tf)))); return digits.zfill(6)[:6]
            except Exception: pass
        s=str(t).strip()
        if ":" in s or "." in s:
            tt=pd.to_datetime("2000-01-01 "+s.replace(":",":").replace(".",":"), errors="coerce")
            if pd.notna(tt): return tt.strftime("%H%M%S")
        digits=re.sub(r"\D","",s); return (digits.zfill(6)[:6]) if digits!="" else "000000"
    hhmmss=to_hhmmss(time_val)
    return f"{date_ts.strftime('%Y-%m-%d')} {hhmmss[:2]}:{hhmmss[2:4]}:{hhmmss[4:6]}"

def abc_offline_from_file(p: Path) -> pd.DataFrame:
    try:
        xls=pd.ExcelFile(p)
        if "APSH" not in xls.sheet_names: return pd.DataFrame(columns=TEMPLATE_COLS)
        df=xls.parse("APSH", header=0)
    except Exception: return pd.DataFrame(columns=TEMPLATE_COLS)
    if df.empty: return pd.DataFrame(columns=TEMPLATE_COLS)
    df.columns=pd.Index(df.columns).astype(str).str.strip()
    n=len(df); out=pd.DataFrame(index=df.index)
    out["æœ¬æ–¹è´¦å·"]=df.get("è´¦å·","")
    out["æœ¬æ–¹å¡å·"]=df.get("å¡å·","").astype(str).str.replace(r"\.0$","",regex=True)
    out["æŸ¥è¯¢è´¦æˆ·"]=out["æœ¬æ–¹è´¦å·"]; out["æŸ¥è¯¢å¡å·"]=out["æœ¬æ–¹å¡å·"]
    holder=df.get("æˆ·å",""); holder = pd.Series([holder]*n,index=df.index) if not isinstance(holder,pd.Series) else holder
    out["æŸ¥è¯¢å¯¹è±¡"]=holder.fillna("").astype(str).str.strip().replace({"nan":""}).replace("","æœªçŸ¥")
    out["åé¦ˆå•ä½"]="å†œä¸šé“¶è¡Œ"; out["å¸ç§"]="CNY"
    amt=pd.to_numeric(df.get("äº¤æ˜“é‡‘é¢",0), errors="coerce"); out["äº¤æ˜“é‡‘é¢"]=amt
    out["è´¦æˆ·ä½™é¢"]=pd.to_numeric(df.get("äº¤æ˜“åä½™é¢",""), errors="coerce")
    out["å€Ÿè´·æ ‡å¿—"]=np.where(amt>0,"è¿›",np.where(amt<0,"å‡º",""))
    dates=df.get("äº¤æ˜“æ—¥æœŸ",""); times=df.get("äº¤æ˜“æ—¶é—´","")
    out["äº¤æ˜“æ—¶é—´"]=[_merge_abc_datetime(d,t) for d,t in zip(dates,times)]
    out["äº¤æ˜“æ‘˜è¦"]=df.get("æ‘˜è¦","").astype(str); out["äº¤æ˜“æµæ°´å·"]=""
    out["äº¤æ˜“ç±»å‹"]=""
    out["äº¤æ˜“å¯¹æ–¹å§“å"]=df.get("å¯¹æ–¹æˆ·å"," ").astype(str)
    out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"]=df.get("å¯¹æ–¹è´¦å·"," ").astype(str)
    out["äº¤æ˜“å¯¹æ–¹å¡å·"]=""
    out["äº¤æ˜“å¯¹æ–¹è¯ä»¶å·ç "]=" "; out["äº¤æ˜“å¯¹æ‰‹ä½™é¢"]=""
    out["äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ"]=df.get("å¯¹æ–¹å¼€æˆ·è¡Œ"," ").astype(str)
    out["äº¤æ˜“ç½‘ç‚¹åç§°"]=df.get("äº¤æ˜“ç½‘ç‚¹","").astype(str)
    out["äº¤æ˜“ç½‘ç‚¹ä»£ç "]=df.get("äº¤æ˜“è¡Œå·","").astype(str)
    out["æ—¥å¿—å·"]=""
    out["ä¼ ç¥¨å·"]=df.get("ä¼ ç¥¨å·","").astype(str)
    out["å‡­è¯ç§ç±»"]=""
    out["å‡­è¯å·"]=""
    out["ç°é‡‘æ ‡å¿—"]=""
    out["ç»ˆç«¯å·"]=df.get("äº¤æ˜“æ¸ é“","").astype(str)
    out["äº¤æ˜“æ˜¯å¦æˆåŠŸ"]=""
    out["äº¤æ˜“å‘ç”Ÿåœ°"]=""
    out["å•†æˆ·åç§°"]=""
    out["å•†æˆ·å·"]=""
    out["IPåœ°å€"]=""
    out["MAC"]=""
    out["äº¤æ˜“æŸœå‘˜å·"]=""
    out["å¤‡æ³¨"]=""
    return out.reindex(columns=TEMPLATE_COLS, fill_value="")

# ---- å»ºè¡Œçº¿ä¸‹ äº¤æ˜“æ˜ç»† ----
def _is_ccb_offline_file(p: Path) -> bool:
    try:
        xls=pd.ExcelFile(p)
        if "äº¤æ˜“æ˜ç»†" not in xls.sheet_names: return False
        df_head=xls.parse("äº¤æ˜“æ˜ç»†", nrows=1)
        cols=set(map(str, df_head.columns))
        return {"å®¢æˆ·åç§°","è´¦å·","äº¤æ˜“æ—¥æœŸ","äº¤æ˜“æ—¶é—´","äº¤æ˜“é‡‘é¢"}.issubset(cols)
    except Exception: return False

def ccb_offline_from_file(p: Path) -> pd.DataFrame:
    try:
        xls=pd.ExcelFile(p)
        if "äº¤æ˜“æ˜ç»†" not in xls.sheet_names: return pd.DataFrame(columns=TEMPLATE_COLS)
        df=xls.parse("äº¤æ˜“æ˜ç»†", header=0)
    except Exception: return pd.DataFrame(columns=TEMPLATE_COLS)
    if df.empty: return pd.DataFrame(columns=TEMPLATE_COLS)
    df.columns=pd.Index(df.columns).astype(str).str.strip()
    out=pd.DataFrame(index=df.index)
    out["æœ¬æ–¹è´¦å·"]=df.get("è´¦å·","")
    out["æœ¬æ–¹å¡å·"]=df.get("äº¤æ˜“å¡å·","").astype(str).str.replace(r"\.0$","",regex=True)
    out["æŸ¥è¯¢è´¦æˆ·"]=out["æœ¬æ–¹è´¦å·"]; out["æŸ¥è¯¢å¡å·"]=out["æœ¬æ–¹å¡å·"]
    out["æŸ¥è¯¢å¯¹è±¡"]=df.get("å®¢æˆ·åç§°","").astype(str).replace({"nan":""}).replace("","æœªçŸ¥")
    out["åé¦ˆå•ä½"]="å»ºè®¾é“¶è¡Œ"
    out["å¸ç§"]=df.get("å¸ç§","CNY").astype(str).replace({"äººæ°‘å¸":"CNY","äººæ°‘å¸å…ƒ":"CNY","RMB":"CNY","156":"CNY"}).fillna("CNY")
    amt=pd.to_numeric(df.get("äº¤æ˜“é‡‘é¢",0), errors="coerce"); out["äº¤æ˜“é‡‘é¢"]=amt
    out["è´¦æˆ·ä½™é¢"]=pd.to_numeric(df.get("è´¦æˆ·ä½™é¢",""), errors="coerce")
    jd=df.get("å€Ÿè´·æ–¹å‘","").astype(str).str.strip()
    out["å€Ÿè´·æ ‡å¿—"]=np.where(jd.str.contains("^è´·",na=False)|jd.str.upper().isin(["è´·","C","CR","CREDIT"]),"è¿›",
                        np.where(jd.str.contains("^å€Ÿ",na=False)|jd.str.upper().isin(["å€Ÿ","D","DR","DEBIT"]),"å‡º",
                                 np.where(amt>0,"è¿›",np.where(amt<0,"å‡º",""))))
    dates=df.get("äº¤æ˜“æ—¥æœŸ",""); times=df.get("äº¤æ˜“æ—¶é—´",""); times_str=pd.Series(times).astype(str).str.replace(r"\.0$","",regex=True)
    out["äº¤æ˜“æ—¶é—´"]=[_parse_dt(d,t,False) for d,t in zip(dates,times_str)]
    out["äº¤æ˜“æ‘˜è¦"]=df.get("æ‘˜è¦"," ").astype(str); out["äº¤æ˜“ç±»å‹"]=""
    out["äº¤æ˜“æµæ°´å·"]=df.get("äº¤æ˜“æµæ°´å·","").astype(str)
    out["äº¤æ˜“å¯¹æ–¹å§“å"]=df.get("å¯¹æ–¹æˆ·å"," ").astype(str)
    out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"]=df.get("å¯¹æ–¹è´¦å·"," ").astype(str)
    out["äº¤æ˜“å¯¹æ–¹å¡å·"]=""
    out["äº¤æ˜“å¯¹æ–¹è¯ä»¶å·ç "]=" "; out["äº¤æ˜“å¯¹æ‰‹ä½™é¢"]=""
    out["äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ"]=df.get("å¯¹æ–¹è¡Œå"," ").astype(str)
    out["äº¤æ˜“ç½‘ç‚¹åç§°"]=df.get("äº¤æ˜“æœºæ„åç§°","").astype(str)
    out["äº¤æ˜“ç½‘ç‚¹ä»£ç "]=df.get("äº¤æ˜“æœºæ„å·","").astype(str)
    out["äº¤æ˜“æŸœå‘˜å·"]=df.get("æŸœå‘˜å·","").astype(str)
    out["ç»ˆç«¯å·"]=df.get("äº¤æ˜“æ¸ é“","").astype(str)
    ext=df.get("æ‰©å……å¤‡æ³¨","").astype(str).replace({"nan":""}); out["å¤‡æ³¨"]=ext
    out["ç°é‡‘æ ‡å¿—"]=""; out["æ—¥å¿—å·"]=""; out["ä¼ ç¥¨å·"]=""
    out["å‡­è¯ç§ç±»"]=""; out["å‡­è¯å·"]=""
    out["äº¤æ˜“æ˜¯å¦æˆåŠŸ"]=""
    out["äº¤æ˜“å‘ç”Ÿåœ°"]=""
    out["å•†æˆ·åç§°"]=df.get("å•†æˆ·åç§°","").astype(str)
    out["å•†æˆ·å·"]=df.get("å•†æˆ·å·","").astype(str)
    out["IPåœ°å€"]=df.get("IPåœ°å€","").astype(str)
    out["MAC"]=df.get("MACåœ°å€","").astype(str)
    return out.reindex(columns=TEMPLATE_COLS, fill_value="")

# ------------------------------------------------------------------
# é€šè®¯å½•è¯»å–ï¼ˆä¿®å¤ç‰ˆï¼‰â€”â€” è‡ªåŠ¨æ¢æµ‹è¡¨å¤´ï¼›æ‰‹æœºå·æå–+å…œåº•æ‰«æ
# ------------------------------------------------------------------
CONTACT_NAME_COLS = ["å§“å","è”ç³»äºº","äººå‘˜å§“å","å§“å/åç§°"]
CONTACT_DEPT_KEYS = ["å®é™…å·¥ä½œå•ä½"]                     # å›ºå®š
CONTACT_TITLE_KEYS = ["èŒåŠ¡","èŒåŠ¡æˆ–å²—ä½","å²—ä½"]         # æ–°å¢â€œèŒåŠ¡æˆ–å²—ä½â€

# ã€å…³é”®ä¿®å¤ã€‘â€”â€” æ‰©å……å¯ä½œä¸ºâ€œå·ç åˆ—â€çš„å…³é”®å­—
CONTACT_PHONE_KEYS = [
    "å·ç ","æ‰‹æœºå·","æ‰‹æœºå·ç ","è”ç³»ç”µè¯","ç”µè¯","è”ç³»æ–¹å¼","è”ç³»å·ç ","ç§»åŠ¨ç”µè¯","è”ç³»æ‰‹æœº","è”ç³»ç”µè¯ï¼ˆæ‰‹æœºï¼‰","æ‰‹æœº"
]

def _guess_header_row(xls: pd.ExcelFile, sheet_name: str, scan_rows: int = 30) -> int:
    df0 = xls.parse(sheet_name, header=None, nrows=scan_rows)
    for i, row in df0.iterrows():
        if row.astype(str).str.contains("å§“å|å·ç |è”ç³»ç”µè¯|ç”µè¯|æ‰‹æœºå·|èŒåŠ¡|å²—ä½|å®é™…å·¥ä½œå•ä½|è”ç³»æ–¹å¼").any():
            return i
    return 0

def _compose_title(dept: Any, title: Any) -> str:
    d = "" if pd.isna(dept) else str(dept).strip()
    t = "" if pd.isna(title) else str(title).strip()
    if d and t: return f"{d}-{t}"
    if t: return t
    if d: return d
    return ""

def _find_first_col(df: pd.DataFrame, keys: List[str]) -> Optional[str]:
    for c in df.columns:
        cs = str(c).strip()
        for k in keys:
            if k in cs:
                return c
    return None

def _extract_mobile_from_row(row: pd.Series) -> str:
    """å…œåº•ï¼šåœ¨æ•´è¡Œæ–‡æœ¬é‡Œæœç¬¬ä¸€ä¸ªå¤§é™†æ‰‹æœºå·"""
    text = " ".join(map(lambda v: "" if pd.isna(v) else str(v), row.values))
    m = _MOBILE_PAT.search(text)
    return m.group(1) if m else ""

def load_contacts_phone_map(root: Path) -> Dict[str, Tuple[str,str]]:
    # å›ºå®šæç¤º
    print("æ­£åœ¨è¯»å–é€šè®¯å½•......")

    def _is_in_out_dir(p: Path) -> bool:
        try:
            return OUT_DIR is not None and p.resolve().is_relative_to(OUT_DIR.resolve())
        except AttributeError:
            return OUT_DIR is not None and str(p.resolve()).startswith(str(OUT_DIR.resolve()))

    # â˜… 1) å…ˆåŠ å…¥â€œå†…ç½®-é€šè®¯å½•.xlsx/.xlsâ€ï¼ˆå§‹ç»ˆè¯»å–ï¼‰
    builtin_files = _iter_builtin_contacts_files()
    if builtin_files:
        for bp in builtin_files:
            print(f"  â€¢ ä½¿ç”¨å†…ç½®é€šè®¯å½•ï¼š{bp.name}")

    # â˜… 2) å†æ”¶é›†å·¥ä½œç›®å½•ä¸‹æ™®é€šé€šè®¯å½•ï¼ˆè·³è¿‡å·²æ ‡æ³¨/è¾“å‡ºç›®å½•ï¼‰
    repo_files = [
        p for p in root.rglob("*é€šè®¯å½•*.xls*")
        if ("å·²æ ‡æ³¨" not in p.stem) and (not _is_in_out_dir(p))
    ]

    # åˆå¹¶å¹¶å»é‡ï¼ˆæŒ‰çœŸå®è·¯å¾„ï¼‰
    all_files: List[Path] = []
    seen: set[str] = set()
    for p in [*builtin_files, *repo_files]:
        try:
            rp = str(p.resolve())
        except Exception:
            rp = str(p)
        if rp not in seen:
            all_files.append(p)
            seen.add(rp)

    if not all_files:
        print("â„¹ï¸ æœªå‘ç°å¯ç”¨çš„é€šè®¯å½•ã€‚")
        return {}

    merged: Dict[str, Tuple[str,str]] = {}
    total_rows = 0
    total_with_phone = 0

    for p in all_files:
        try:
            xls = pd.ExcelFile(p)
        except Exception as e:
            print("âŒ é€šè®¯å½•è½½å…¥å¤±è´¥", p.name, e); 
            continue

        for sht in xls.sheet_names:
            try:
                # 1) æ¢æµ‹è¡¨å¤´
                hdr_row = _guess_header_row(xls, sht, 30)

                # 2) å…ˆè¯»ä¸€éæ‹¿åˆ°åˆ—å
                df0 = xls.parse(sht, header=hdr_row)
                df0.columns = pd.Index(df0.columns).astype(str).str.strip()

                # 3) æ‰¾å…³é”®åˆ—å
                def _find_col(keys: List[str]) -> Optional[str]:
                    for c in df0.columns:
                        cs = str(c).strip()
                        for k in keys:
                            if k in cs:
                                return c
                    return None

                name_col  = _find_col(CONTACT_NAME_COLS)
                phone_col = _find_col(CONTACT_PHONE_KEYS)
                dept_col  = _find_col(CONTACT_DEPT_KEYS)
                title_col = _find_col(CONTACT_TITLE_KEYS)

                # 4) å¼ºåˆ¶æŠŠã€å·ç åˆ—ã€‘æŒ‰å­—ç¬¦ä¸²é‡è¯»ï¼›è‹¥æ‰¾ä¸åˆ°å·ç åˆ—ï¼Œä¹Ÿå…ˆæŒ‰æ™®é€šè¯»ï¼Œåç»­å…œåº•æ‰«æ
                dtype_kw = {phone_col: str} if phone_col else None
                df = xls.parse(sht, header=hdr_row, dtype=dtype_kw)
                df.columns = pd.Index(df.columns).astype(str).str.strip()

                total_rows += len(df.index)

                nm_ser   = df[name_col].astype(str).str.strip() if name_col else pd.Series([""]*len(df))
                dept_ser = df[dept_col] if dept_col else pd.Series([""]*len(df))
                titl_ser = df[title_col] if title_col else pd.Series([""]*len(df))

                if phone_col:
                    raw_phone = df[phone_col]
                    phone_ser = raw_phone.map(normalize_phone_cell)
                else:
                    # æ²¡æœ‰æ˜¾å¼å·ç åˆ—ï¼šå¯¹æ•´è¡Œåšæ‰‹æœºå·æ‰«æ
                    phone_ser = df.apply(_extract_mobile_from_row, axis=1)

                # sheet çº§ç»Ÿè®¡
                sheet_phones = phone_ser.astype(bool).sum()
                total_with_phone += int(sheet_phones)
                print(f"  â€¢ é€šè®¯å½• {p.name} / {sht}: è¡Œæ•° {len(df)}, å‘½ä¸­æ‰‹æœºå· {int(sheet_phones)}")

                for nm, ph, dp, tt in zip(nm_ser, phone_ser, dept_ser, titl_ser):
                    if not ph:
                        continue
                    job = _compose_title(dp, tt)  # å®é™…å·¥ä½œå•ä½-ï¼ˆèŒåŠ¡/èŒåŠ¡æˆ–å²—ä½/å²—ä½ï¼‰
                    if ph not in merged:
                        merged[ph] = (nm, job)
                    else:
                        old_nm, old_job = merged[ph]
                        merged[ph] = (old_nm or nm, old_job or job)

            except Exception as e:
                print("âŒ é€šè®¯å½•è§£æå¤±è´¥", f"{p.name}->{sht}", e)

    print(f"âœ… é€šè®¯å½•å·ç æ˜ å°„åŠ è½½å®Œæˆï¼š{len(merged)} æ¡ï¼ˆæ‰«æè¡Œæ•° {total_rows}ï¼›å«æ‰‹æœºå· {total_with_phone}ï¼‰ã€‚")
    # ä¸è¾“å‡ºæ ·ä¾‹
    return merged


# ------------------------------------------------------------------
# é€šä¿¡ï¼šå·ç åŒ¹é…é€šè®¯å½• â†’ å›å†™é€šä¿¡â€œå§“åâ€â€œèŒåŠ¡â€ï¼›è¾“å‡º é€šä¿¡â€œå§“åâ†’èŒåŠ¡â€
# ------------------------------------------------------------------
CALLLOG_PHONE_COL_CANDS = ["å¯¹æ–¹å·ç "]
CALLLOG_NAME_COL_CANDS  = ["å§“å","å¯¹æ–¹å§“å","è”ç³»äºº","åç§°","å®¢æˆ·åç§°","æˆ·å"]

def _find_col_any(df: pd.DataFrame, cands: List[str]) -> Optional[str]:
    for c in map(str, df.columns):
        sc = c.strip()
        for key in cands:
            if key in sc:
                return c
    return None

def _enrich_one_comm_df(df: pd.DataFrame, phone_to_name_title: Dict[str, Tuple[str,str]]) -> Tuple[pd.DataFrame, Dict[str,str]]:
    if df is None or df.empty: return pd.DataFrame(), {}
    d = df.copy(); d.columns = pd.Index(d.columns).astype(str).str.strip()
    phone_col = _find_col_any(d, CALLLOG_PHONE_COL_CANDS)
    if not phone_col:
        return d, {}
    phones = d[phone_col].map(normalize_phone_cell)

    names = []
    titles = []
    for ph in phones:
        nm, job = phone_to_name_title.get(ph, ("",""))
        names.append(nm)
        titles.append(job)
    names  = pd.Series(names, index=d.index)
    titles = pd.Series(titles, index=d.index)

    name_col_existing = _find_col_any(d, CALLLOG_NAME_COL_CANDS)
    if name_col_existing:
        d["å§“å"] = np.where(names!="", names, d[name_col_existing].astype(str).str.strip())
    else:
        d["å§“å"] = names

    d["èŒåŠ¡"] = titles  # å·ç å‘½ä¸­åˆ™æœ‰å€¼ï¼Œå¦åˆ™ä¸ºç©º

    tmp = d[["å§“å","èŒåŠ¡"]].copy()
    tmp = tmp[(tmp["å§“å"]!="") & (~tmp["å§“å"].str.lower().eq("nan")) & (tmp["èŒåŠ¡"]!="")]
    map_name_title: Dict[str,str] = {}
    for name, sub in tmp.groupby("å§“å"):
        uniq = list(dict.fromkeys(sub["èŒåŠ¡"].astype(str).tolist()))
        map_name_title[name] = "ã€".join(x for x in uniq if x)
    return d, map_name_title

def load_and_enrich_communications(root: Path, phone_to_name_title: Dict[str, Tuple[str,str]]) -> Dict[str,str]:
    if not phone_to_name_title:
        print("â„¹ï¸ æœªèƒ½ä»é€šè®¯å½•ç”Ÿæˆå·ç æ˜ å°„ï¼Œè·³è¿‡é€šä¿¡æ ‡æ³¨ã€‚")
        return {}

    def _is_in_out_dir(p: Path) -> bool:
        try:
            return OUT_DIR is not None and p.resolve().is_relative_to(OUT_DIR.resolve())
        except AttributeError:
            # Python<3.9 æ²¡æœ‰ is_relative_to
            return OUT_DIR is not None and str(p.resolve()).startswith(str(OUT_DIR.resolve()))

    # â˜… ä»…è¯»å–åŸå§‹â€œé€šä¿¡â€æ–‡ä»¶ï¼›è·³è¿‡â€œå·²æ ‡æ³¨â€ä¸è¾“å‡ºç›®å½•é‡Œçš„æ–‡ä»¶
    files = [
        p for p in root.rglob("*.xlsx")
        if ("é€šä¿¡" in p.stem or "é€šä¿¡" in p.name)
        and ("å·²æ ‡æ³¨" not in p.stem)
        and (not _is_in_out_dir(p))
    ]

    if not files:
        print("â„¹ï¸ æœªå‘ç°æ–‡ä»¶ååŒ…å«â€œé€šä¿¡â€çš„ .xlsxã€‚")
        return {}
    out_all: Dict[str,str] = {}
    for p in files:
        print(f"ğŸ“ é€šä¿¡åŒ¹é…ï¼š{p.name} ...")
        try:
            xls = pd.ExcelFile(p)
        except Exception as e:
            print("âŒ é€šä¿¡æ–‡ä»¶è½½å…¥å¤±è´¥", p.name, e); continue
        frames = []; name_map_file: Dict[str,str] = {}
        for sht in xls.sheet_names:
            try:
                df0 = xls.parse(sheet_name=sht, header=0)
            except Exception as e:
                print("âŒ é€šä¿¡è§£æå¤±è´¥", f"{p.name}->{sht}", e); continue
            enriched, name_map = _enrich_one_comm_df(df0, phone_to_name_title)
            if not enriched.empty:
                enriched.insert(0,"__æ¥æºsheet__",sht); frames.append(enriched)
            for k,v in name_map.items():
                if k in name_map_file and name_map_file[k]:
                    exist = name_map_file[k].split("ã€")
                    add = [x for x in v.split("ã€") if x not in exist]
                    name_map_file[k] = "ã€".join(exist + add)
                else:
                    name_map_file[k] = v
        if frames:
            merged = pd.concat(frames, ignore_index=True)
            save_df_auto_width(merged, Path("é€šä¿¡-å·²æ ‡æ³¨")/f"{p.stem}-å·²æ ‡æ³¨", index=False, engine="openpyxl")
            print(f"âœ… é€šä¿¡å·²æ ‡æ³¨å¯¼å‡ºï¼š{p.stem}-å·²æ ‡æ³¨.xlsx")
        for k,v in name_map_file.items():
            if k in out_all and out_all[k]:
                exist = out_all[k].split("ã€")
                add = [x for x in v.split("ã€") if x not in exist]
                out_all[k] = "â€¢".join(exist)  # ä¸é‡è¦ï¼›åˆå¹¶ç­–ç•¥åŒä¸Š
                out_all[k] = "ã€".join(exist + add)
            else:
                out_all[k] = v
    print(f"âœ… é€šä¿¡å§“åæ˜ å°„ç”Ÿæˆ {len(out_all)} æ¡ã€‚")
    return out_all

# ------------------------------------------------------------------
# åˆå¹¶å…¨éƒ¨æµæ°´ï¼ˆåœ¨å¯¼å‡ºå‰æŠŠâ€œå¯¹æ–¹èŒåŠ¡â€æ¥ä¸Šï¼‰
# ------------------------------------------------------------------
def merge_all_txn(root_dir: str) -> pd.DataFrame:
    root = Path(root_dir).expanduser().resolve()

    # 1) é€šè®¯å½•ï¼šæ‰‹æœºå· -> (å§“å, èŒåŠ¡)
    global CONTACT_PHONE_TO_NAME_TITLE, CALLLOG_NAME_TO_TITLE
    CONTACT_PHONE_TO_NAME_TITLE = load_contacts_phone_map(root)

    # 2) é€šä¿¡ï¼šä»¥å·ç åŒ¹é…é€šè®¯å½•ï¼Œå›å†™é€šä¿¡ä¸­çš„â€œå§“åâ€â€œèŒåŠ¡â€ï¼Œå¹¶æ±‡æ€»â€œå§“åâ†’èŒåŠ¡â€
    CALLLOG_NAME_TO_TITLE = load_and_enrich_communications(root, CONTACT_PHONE_TO_NAME_TITLE)

    # 3) æ”¶é›†é“¶è¡Œä¸å…¶ä»–æ¥æºæ–‡ä»¶
    china_files = [p for p in root.rglob("*-*-äº¤æ˜“æµæ°´.xls*")]
    all_excel = list(root.rglob("*.xls*"))
    rc_candidates = [p for p in all_excel if "å†œå•†è¡Œ" in p.as_posix()]
    pattern_old = re.compile(r"è€\s*[è´¦å¸]\s*(?:å·|æˆ·)")
    old_rc = [p for p in rc_candidates if pattern_old.search(p.stem)]
    new_rc = [p for p in rc_candidates if p not in old_rc]
    tl_files = [p for p in all_excel if "æ³°éš†" in p.as_posix()]
    mt_files = [p for p in all_excel if "æ°‘æ³°" in p.as_posix()]
    abc_offline_files = [p for p in all_excel if _is_abc_offline_file(p)]
    ccb_offline_files = [p for p in all_excel if _is_ccb_offline_file(p)]
    csv_txn_files = [p for p in root.rglob("äº¤æ˜“æ˜ç»†ä¿¡æ¯.csv")]

    print(f"âœ… ç½‘ä¸Šé“¶è¡Œ {len(china_files)}ï¼Œè€å†œå•† {len(old_rc)}ï¼Œæ–°å†œå•† {len(new_rc)}ï¼Œæ³°éš† {len(tl_files)}ï¼Œæ°‘æ³° {len(mt_files)}ï¼Œå†œè¡Œçº¿ä¸‹ {len(abc_offline_files)}ï¼Œå»ºè¡Œçº¿ä¸‹ {len(ccb_offline_files)}ï¼ŒCSV {len(csv_txn_files)}ï¼›é€šä¿¡æ˜ å°„ {len(CALLLOG_NAME_TO_TITLE)} æ¡ã€‚")

    dfs: List[pd.DataFrame] = []

    # ç½‘é“¶æ ‡å‡†
    for p in china_files:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        try:
            df = pd.read_excel(p, dtype={"æŸ¥è¯¢å¡å·":str,"æŸ¥è¯¢è´¦æˆ·":str,"äº¤æ˜“å¯¹æ–¹è¯ä»¶å·ç ":str,"æœ¬æ–¹è´¦å·":str,"æœ¬æ–¹å¡å·":str})
            df["æ¥æºæ–‡ä»¶"] = p.name
            dfs.append(df)
        except Exception as e:
            print("âŒ", p.name, e)

    # å†œå•†è¡Œ
    for p in old_rc + new_rc:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        kw = should_skip_special(p)
        if kw:
            print(f"â© è·³è¿‡ã€{p.name}ã€‘ï¼šè¡¨å¤´å«â€œ{kw}â€"); continue
        raw = _read_raw(p)
        holder = extract_holder_from_df(raw) or holder_from_folder(p.parent) or fallback_holder_from_path(p)
        is_old = p in old_rc
        df = rc_to_template(raw, holder, is_old)
        if not df.empty:
            df["æ¥æºæ–‡ä»¶"] = p.name; dfs.append(df)

    # æ³°éš†
    for p in tl_files:
        if "å¼€æˆ·" in p.stem: continue
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        try:
            xls = pd.ExcelFile(p)
        except Exception as e:
            print("âŒ", f"{p.name} è½½å…¥å¤±è´¥", e); continue
        try:
            header_idx = _header_row(p)
        except Exception:
            header_idx = 0
        xls_dict={}
        for sht in xls.sheet_names:
            try:
                df_sheet = xls.parse(sheet_name=sht, header=header_idx)
                xls_dict[sht]=df_sheet
            except Exception as e:
                print("âŒ", f"{p.name}->{sht}", e)
        df = tl_to_template(xls_dict)
        if not df.empty:
            df["æ¥æºæ–‡ä»¶"]=p.name; dfs.append(df)

    # æ°‘æ³°
    for p in mt_files:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        raw = _read_raw(p); df = mt_to_template(raw)
        if not df.empty:
            df["æ¥æºæ–‡ä»¶"]=p.name; dfs.append(df)

    # å†œè¡Œçº¿ä¸‹
    for p in abc_offline_files:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        try:
            df=abc_offline_from_file(p)
            if not df.empty:
                df["æ¥æºæ–‡ä»¶"]=p.name; dfs.append(df)
        except Exception as e:
            print("âŒ å†œè¡Œçº¿ä¸‹è§£æå¤±è´¥", p.name, e)

    # å»ºè¡Œçº¿ä¸‹
    for p in ccb_offline_files:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        try:
            df=ccb_offline_from_file(p)
            if not df.empty:
                df["æ¥æºæ–‡ä»¶"]=p.name; dfs.append(df)
        except Exception as e:
            print("âŒ å»ºè¡Œçº¿ä¸‹è§£æå¤±è´¥", p.name, e)

    # CSV
    for p in csv_txn_files:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        try:
            raw_csv = _read_csv_smart(p)
        except Exception as e:
            print("âŒ æ— æ³•è¯»å–CSV", p.name, e); continue
        holder = _person_from_people_csv(p.parent) or holder_from_folder(p.parent) or fallback_holder_from_path(p)
        feedback_unit = p.parent.name
        try:
            df = csv_to_template(raw_csv, holder, feedback_unit)
        except Exception as e:
            print("âŒ CSVè½¬æ¨¡æ¿å¤±è´¥", p.name, e); continue
        if not df.empty:
            df["æ¥æºæ–‡ä»¶"]=p.name; dfs.append(df)

    print("æ–‡ä»¶è¯»å–å®Œæˆï¼Œæ­£åœ¨æ•´åˆâ€¦â€¦")
    if not dfs:
        return pd.DataFrame(columns=TEMPLATE_COLS)

    all_txn = pd.concat(dfs, ignore_index=True)

    # å»é‡ï¼šæµæ°´å·+æ—¶é—´+é‡‘é¢
    all_txn["äº¤æ˜“é‡‘é¢"] = pd.to_numeric(all_txn["äº¤æ˜“é‡‘é¢"], errors="coerce").round(2)
    before=len(all_txn)
    all_txn = all_txn.drop_duplicates(subset=["äº¤æ˜“æµæ°´å·","äº¤æ˜“æ—¶é—´","äº¤æ˜“é‡‘é¢"], keep="first").reset_index(drop=True)
    removed=before-len(all_txn)
    if removed: print(f"ğŸ§¹ å»é‡ {removed} æ¡ã€‚")

    # æ’åº/åºå·
    ts = pd.to_datetime(all_txn["äº¤æ˜“æ—¶é—´"], errors="coerce")
    all_txn.insert(0,"__ts__",ts)
    all_txn.sort_values("__ts__", inplace=True, kind="mergesort")
    all_txn["åºå·"] = range(1, len(all_txn)+1)
    all_txn.drop(columns="__ts__", inplace=True)

    # æ ‡å‡†åŒ–ã€åˆ†ç®±ã€æ˜ŸæœŸ/èŠ‚å‡æ—¥
    all_txn["å€Ÿè´·æ ‡å¿—"]=all_txn["å€Ÿè´·æ ‡å¿—"].apply(lambda x: "å‡º" if str(x).strip() in {"1","å€Ÿ","D"} else ("è¿›" if str(x).strip() in {"2","è´·","C"} else str(x)))
    bins=[-np.inf,2000,5000,20000,50000,np.inf]; labels=["2000ä»¥ä¸‹","2000-5000","5000-20000","20000-50000","50000ä»¥ä¸Š"]
    all_txn["é‡‘é¢åŒºé—´"]=pd.cut(pd.to_numeric(all_txn["äº¤æ˜“é‡‘é¢"], errors="coerce"), bins=bins, labels=labels, right=False, include_lowest=True)
    weekday_map={0:"æ˜ŸæœŸä¸€",1:"æ˜ŸæœŸäºŒ",2:"æ˜ŸæœŸä¸‰",3:"æ˜ŸæœŸå››",4:"æ˜ŸæœŸäº”",5:"æ˜ŸæœŸå…­",6:"æ˜ŸæœŸæ—¥"}
    wk = pd.Series(index=all_txn.index, dtype=object); mask=ts.notna()
    wk.loc[mask]=ts.dt.weekday.map(weekday_map); wk.loc[~mask]="wrong"; all_txn["æ˜ŸæœŸ"]=wk
    dates=ts.dt.date; status=pd.Series(index=all_txn.index, dtype=object)
    unique_dates=pd.unique(dates[mask])
    @lru_cache(maxsize=None)
    def _day_status(d)->str:
        try: return "èŠ‚å‡æ—¥" if is_holiday(d) else ("å·¥ä½œæ—¥" if is_workday(d) else "å‘¨æœ«")
        except Exception:
            dd=datetime.datetime.combine(d, datetime.time())
            return "å‘¨æœ«" if dd.weekday()>=5 else "å·¥ä½œæ—¥"
    if len(unique_dates):
        mapd={d:_day_status(d) for d in unique_dates}; status.loc[mask]=dates.loc[mask].map(mapd)
    status.loc[~mask]="wrong"; all_txn["èŠ‚å‡æ—¥"]=status

    # â€”â€” å¯¹æ–¹èŒåŠ¡ï¼šä¼˜å…ˆâ€œé€šä¿¡å§“åâ†’èŒåŠ¡â€ï¼Œâ˜… è‹¥æ— åˆ™å›é€€â€œé€šè®¯å½•å§“åâ†’èŒåŠ¡â€
    #   a) é€šä¿¡å§“åâ†’èŒåŠ¡ï¼ˆå·²æœ‰ï¼‰
    name_from_comm = CALLLOG_NAME_TO_TITLE or {}

    #   b) é€šè®¯å½•å§“åâ†’èŒåŠ¡ï¼ˆå°†æ‰‹æœºå·æ˜ å°„åå‘ï¼šå§“å -> èŒåŠ¡ï¼‰ï¼Œå–ç¬¬ä¸€ä¸ªéç©ºèŒåŠ¡
    contacts_name_to_title: Dict[str, str] = {}
    for ph, (nm, job) in CONTACT_PHONE_TO_NAME_TITLE.items():
        if not nm:
            continue
        if nm not in contacts_name_to_title:
            contacts_name_to_title[nm] = job or ""
        else:
            # ç©ºåˆ™ç”¨æ–°çš„éç©ºè¦†ç›–
            if not contacts_name_to_title[nm] and job:
                contacts_name_to_title[nm] = job

    #   c) æœ€ç»ˆæ˜ å°„ï¼šå…ˆç”¨é€šä¿¡ï¼Œç¼ºå¤±å†ç”¨é€šè®¯å½•
    final_title_by_name = name_from_comm.copy()
    for nm, job in contacts_name_to_title.items():
        if nm not in final_title_by_name or not final_title_by_name[nm]:
            final_title_by_name[nm] = job

    all_txn["å¯¹æ–¹èŒåŠ¡"] = (
        all_txn["äº¤æ˜“å¯¹æ–¹å§“å"]
        .map(final_title_by_name)
        .fillna("")
    )

    save_df_auto_width(all_txn, "æ‰€æœ‰äºº-åˆå¹¶äº¤æ˜“æµæ°´", index=False, engine="openpyxl")
    print("âœ… å·²å¯¼å‡ºï¼šæ‰€æœ‰äºº-åˆå¹¶äº¤æ˜“æµæ°´.xlsx")
    return all_txn

# ------------------------------------------------------------------
# åˆ†æï¼ˆå¯¹æ–¹èŒåŠ¡ç›´æ¥ä½¿ç”¨åˆå¹¶ç»“æœï¼‰
# ------------------------------------------------------------------
def analysis_txn(df: pd.DataFrame) -> None:
    if df.empty: return
    df=df.copy()
    df["äº¤æ˜“æ—¶é—´"]=pd.to_datetime(df["äº¤æ˜“æ—¶é—´"], errors="coerce")
    df["äº¤æ˜“é‡‘é¢"]=pd.to_numeric(df["äº¤æ˜“é‡‘é¢"], errors="coerce")
    person=df["æŸ¥è¯¢å¯¹è±¡"].iat[0] or "æœªçŸ¥"
    prefix=f"{person}/"

    out_df=df[df["å€Ÿè´·æ ‡å¿—"]=="å‡º"]; in_df=df[df["å€Ÿè´·æ ‡å¿—"]=="è¿›"]; counts=df["é‡‘é¢åŒºé—´"].value_counts()
    summary=pd.DataFrame([{
        "äº¤æ˜“æ¬¡æ•°":len(df),
        "äº¤æ˜“é‡‘é¢":df["äº¤æ˜“é‡‘é¢"].sum(skipna=True),
        "æµå‡ºé¢":out_df["äº¤æ˜“é‡‘é¢"].sum(skipna=True),
        "æµå…¥é¢":in_df["äº¤æ˜“é‡‘é¢"].sum(skipna=True),
        "å•ç¬”æœ€å¤§æ”¯å‡º":out_df["äº¤æ˜“é‡‘é¢"].max(skipna=True),
        "å•ç¬”æœ€å¤§æ”¶å…¥":in_df["äº¤æ˜“é‡‘é¢"].max(skipna=True),
        "å‡€æµå…¥":in_df["äº¤æ˜“é‡‘é¢"].sum(skipna=True)-out_df["äº¤æ˜“é‡‘é¢"].sum(skipna=True),
        "æœ€åäº¤æ˜“æ—¶é—´":df["äº¤æ˜“æ—¶é—´"].max(),
        "0-2åƒæ¬¡æ•°":counts.get("2000ä»¥ä¸‹",0),
        "2åƒ-5åƒæ¬¡æ•°":counts.get("2000-5000",0),
        "5åƒ-2ä¸‡æ¬¡æ•°":counts.get("5000-20000",0),
        "2ä¸‡-5ä¸‡æ¬¡æ•°":counts.get("20000-50000",0),
        "5ä¸‡ä»¥ä¸Šæ¬¡æ•°":counts.get("50000ä»¥ä¸Š",0),
    }])
    save_df_auto_width(summary, f"{prefix}0{person}-èµ„äº§åˆ†æ", index=False, engine="openpyxl")

    cash = df[(df["ç°é‡‘æ ‡å¿—"].astype(str).str.contains("ç°", na=False)
               | (pd.to_numeric(df["ç°é‡‘æ ‡å¿—"], errors="coerce")==1)
               | df["äº¤æ˜“ç±»å‹"].astype(str).str.contains("æŸœé¢|ç°", na=False))
              & (pd.to_numeric(df["äº¤æ˜“é‡‘é¢"], errors="coerce")>=10_000)]
    save_df_auto_width(cash, f"{prefix}1{person}-å­˜å–ç°1ä¸‡ä»¥ä¸Š", index=False, engine="openpyxl")

    big = df[pd.to_numeric(df["äº¤æ˜“é‡‘é¢"], errors="coerce")>=500_000]
    save_df_auto_width(big, f"{prefix}1{person}-å¤§é¢èµ„é‡‘50ä¸‡ä»¥ä¸Š", index=False, engine="openpyxl")

    src=df.copy()
    src["is_in"]=src["å€Ÿè´·æ ‡å¿—"]=="è¿›"
    src["signed_amt"]=pd.to_numeric(src["äº¤æ˜“é‡‘é¢"], errors="coerce")*src["is_in"].map({True:1,False:-1})
    src["in_amt"]=pd.to_numeric(src["äº¤æ˜“é‡‘é¢"], errors="coerce").where(src["is_in"],0)
    src=(src.groupby("äº¤æ˜“å¯¹æ–¹å§“å", dropna=False)
         .agg(äº¤æ˜“é‡‘é¢=("äº¤æ˜“é‡‘é¢","sum"),
              äº¤æ˜“æ¬¡æ•°=("äº¤æ˜“é‡‘é¢","size"),
              æµå…¥é¢=("in_amt","sum"),
              å‡€æµå…¥=("signed_amt","sum"),
              å•ç¬”æœ€å¤§æ”¶å…¥=("in_amt","max"))
         .reset_index())
    total=src["æµå…¥é¢"].sum()
    src["æµå…¥æ¯”%"]=src["æµå…¥é¢"]/total*100 if total else 0
    # å¹¶å…¥å¯¹æ–¹èŒåŠ¡ï¼ˆå·²åŒ…å«é€šä¿¡ä¼˜å…ˆ+é€šè®¯å½•å›é€€ï¼‰
    name_to_title = (df[["äº¤æ˜“å¯¹æ–¹å§“å","å¯¹æ–¹èŒåŠ¡"]].dropna().drop_duplicates().set_index("äº¤æ˜“å¯¹æ–¹å§“å")["å¯¹æ–¹èŒåŠ¡"].to_dict())
    src.insert(1,"å¯¹æ–¹èŒåŠ¡", src["äº¤æ˜“å¯¹æ–¹å§“å"].map(name_to_title).fillna(""))
    src.sort_values("æµå…¥é¢", ascending=False, inplace=True)
    save_df_auto_width(src, f"{prefix}1{person}-èµ„é‡‘æ¥æºåˆ†æ", index=False, engine="openpyxl")

def make_partner_summary(df: pd.DataFrame) -> None:
    if df.empty: return
    person=df["æŸ¥è¯¢å¯¹è±¡"].iat[0] or "æœªçŸ¥"; prefix=f"{person}/"
    d=df.copy()
    d["äº¤æ˜“é‡‘é¢"]=pd.to_numeric(d["äº¤æ˜“é‡‘é¢"], errors="coerce").fillna(0)
    d["is_in"]=d["å€Ÿè´·æ ‡å¿—"]=="è¿›"
    d["abs_amt"]=d["äº¤æ˜“é‡‘é¢"].abs()
    d["signed_amt"]=d["äº¤æ˜“é‡‘é¢"]*d["is_in"].map({True:1,False:-1})
    d["in_amt"]=d["äº¤æ˜“é‡‘é¢"].where(d["is_in"],0)
    d["out_amt"]=d["äº¤æ˜“é‡‘é¢"].where(~d["is_in"],0)
    d["gt10k"]=(d["abs_amt"]>=10_000).astype(int)
    summ=(d.groupby(["æŸ¥è¯¢å¯¹è±¡","äº¤æ˜“å¯¹æ–¹å§“å"], dropna=False)
            .agg(äº¤æ˜“æ¬¡æ•°=("äº¤æ˜“é‡‘é¢","size"),
                 äº¤æ˜“é‡‘é¢=("abs_amt","sum"),
                 ä¸‡å…ƒä»¥ä¸Šäº¤æ˜“æ¬¡æ•°=("gt10k","sum"),
                 å‡€æ”¶å…¥=("signed_amt","sum"),
                 è½¬å…¥ç¬”æ•°=("is_in","sum"),
                 è½¬å…¥é‡‘é¢=("in_amt","sum"),
                 è½¬å‡ºç¬”æ•°=("is_in", lambda x:(~x).sum()),
                 è½¬å‡ºé‡‘é¢=("out_amt","sum"))
            .reset_index()
            .rename(columns={"æŸ¥è¯¢å¯¹è±¡":"å§“å","äº¤æ˜“å¯¹æ–¹å§“å":"å¯¹æ–¹å§“å"}))
    name_to_title=(d[["äº¤æ˜“å¯¹æ–¹å§“å","å¯¹æ–¹èŒåŠ¡"]].drop_duplicates().set_index("äº¤æ˜“å¯¹æ–¹å§“å")["å¯¹æ–¹èŒåŠ¡"].to_dict())
    summ.insert(2,"å¯¹æ–¹èŒåŠ¡", summ["å¯¹æ–¹å§“å"].map(name_to_title).fillna(""))
    total=summ.groupby("å§“å")["äº¤æ˜“é‡‘é¢"].transform("sum")
    summ["äº¤æ˜“å æ¯”%"]=np.where(total>0, summ["äº¤æ˜“é‡‘é¢"]/total*100, 0)
    summ.sort_values(["å§“å","äº¤æ˜“é‡‘é¢"], ascending=[True,False], inplace=True)
    save_df_auto_width(summ, f"{prefix}2{person}-äº¤æ˜“å¯¹æ‰‹åˆ†æ", index=False, engine="openpyxl")

    comp=summ[summ["å¯¹æ–¹å§“å"].astype(str).str.contains("å…¬å¸", na=False)]
    save_df_auto_width(comp, f"{prefix}3{person}-ä¸å…¬å¸ç›¸å…³äº¤æ˜“é¢‘æ¬¡åˆ†æ", index=False, engine="openpyxl")

# ------------------------------------------------------------------
# GUI
# ------------------------------------------------------------------
def create_gui():
    root = tk.Tk()
    root.title("æ¸©å²­çºªå§”äº¤æ˜“æµæ°´æ‰¹é‡åˆ†æå·¥å…·")
    root.minsize(820, 600)

    ttk.Label(root, text="æ¸©å²­çºªå§”äº¤æ˜“æµæ°´æ‰¹é‡åˆ†æå·¥å…·", font=("ä»¿å®‹", 20, "bold")).grid(row=0, column=0, columnspan=3, pady=(15, 0))
    ttk.Label(root, text="Â© æ¸©å²­çºªå§”å…­å®¤ å•æŸ³æ˜Š", font=("å¾®è½¯é›…é»‘", 9)).grid(row=1, column=0, columnspan=3, pady=(0, 6))

    # åŸæ¥æ”¾åœ¨é¡¶éƒ¨çš„â€œæç¤ºâ€ Label åˆ é™¤ï¼Œä¸å†ä½¿ç”¨å•ç‹¬æ ‡ç­¾

    ttk.Label(root, text="å·¥ä½œç›®å½•:").grid(row=2, column=0, sticky="e", padx=8, pady=8)
    path_var = tk.StringVar(value=str(Path.home()))
    ttk.Entry(root, textvariable=path_var, width=60).grid(row=2, column=1, sticky="we", padx=5, pady=8)
    ttk.Button(root, text="æµè§ˆ...", command=lambda: path_var.set(filedialog.askdirectory(title="é€‰æ‹©å·¥ä½œç›®å½•") or path_var.get())).grid(row=2, column=2, padx=5, pady=8)

    # è¾“å‡ºæ 
    log_box = tk.Text(root, width=96, height=18, state="disabled")
    log_box.grid(row=4, column=0, columnspan=3, padx=10, pady=(5,10), sticky="nsew")
    root.columnconfigure(1, weight=1); root.rowconfigure(4, weight=1)

    # â˜… åˆå§‹åŒ–æ—¶å°†æç¤ºå†™å…¥è¾“å‡ºæ 
    tip = (
        "æç¤ºï¼šè‹¥è¦æ–°å¢é€šè®¯å½•ï¼Œè¯·åœ¨å·¥ä½œç›®å½•ä¸‹æ”¾ç½®æ–‡ä»¶åä¸­åŒ…å«â€œé€šè®¯å½•.xlsxâ€çš„æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼šæå››-é€šè®¯å½•.xlsxï¼‰ï¼Œå¹¶è‡³å°‘åŒ…å«ä»¥ä¸‹åˆ—ï¼šå§“åã€å®é™…å·¥ä½œå•ä½ã€å·ç ã€‚"
    )
    log_box.config(state="normal")
    log_box.insert("end", tip + "\n")
    log_box.config(state="disabled")

    def log(msg):
        log_box.config(state="normal")
        log_box.insert("end", f"{datetime.datetime.now():%H:%M:%S}  {msg}\n")
        log_box.config(state="disabled"); log_box.see("end")

    def run(path):
        # â˜… ç‚¹å‡»å¼€å§‹åˆ†æåï¼Œå…ˆæ¸…ç©ºè¾“å‡ºæ ï¼ˆåˆ é™¤æç¤ºï¼‰
        log_box.config(state="normal")
        log_box.delete("1.0", "end")
        log_box.config(state="disabled")

        global OUT_DIR
        OUT_DIR = Path(path).expanduser().resolve() / "æ‰¹é‡åˆ†æç»“æœ"
        OUT_DIR.mkdir(parents=True, exist_ok=True)
        _orig_print = builtins.print
        builtins.print = lambda *a, **k: log(" ".join(map(str, a)))
        try:
            all_txn = merge_all_txn(path)
            if all_txn.empty:
                messagebox.showinfo("å®Œæˆ", "æœªæ‰¾åˆ°å¯åˆ†ææ–‡ä»¶"); return
            for person, df_person in all_txn.groupby("æŸ¥è¯¢å¯¹è±¡", dropna=False):
                print(f"--- åˆ†æ {person} ---")
                analysis_txn(df_person)
                make_partner_summary(df_person)
            messagebox.showinfo("å®Œæˆ", f"å…¨éƒ¨åˆ†æå®Œæˆï¼ç»“æœåœ¨:\n{OUT_DIR}")
        except Exception as e:
            messagebox.showerror("é”™è¯¯", str(e))
        finally:
            builtins.print = _orig_print

    ttk.Button(root, text="å¼€å§‹åˆ†æ", command=lambda: threading.Thread(target=run, args=(path_var.get().strip(),), daemon=True).start(), width=18).grid(row=3, column=1, pady=10)
    root.mainloop()


# ------------------------------------------------------------------
if __name__ == "__main__":
    create_gui()

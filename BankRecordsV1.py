#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
äº¤æ˜“æµæ°´æ‰¹é‡åˆ†æå·¥å…· GUI   v6-plus (refactor + çº¿ä¸‹é“¶è¡Œæ‰©å±•)
Author  : æ¸©å²­çºªå§”å…­å®¤ å•æŸ³æ˜Š   ï¼ˆ2025-08-05 ä¿®è®¢ï¼‰
é‡æ„è€…  : ï¼ˆæ•ˆç‡ä¼˜åŒ–ç‰ˆ 2025-08-28ï¼‰
æ‰©å±•è€…  : ï¼ˆçº¿ä¸‹å†œè¡Œ/å»ºè¡Œæ¥å…¥ 2025-09-09ï¼‰

ï¼ˆ2025-08-27 å¢è¡¥ä¿æŒä¸å˜ï¼‰
- æ–°å¢ï¼šæ”¯æŒè¯»å–åŒç›®å½•ä¸‹å›ºå®šæ–‡ä»¶å â€œäº¤æ˜“æ˜ç»†ä¿¡æ¯.csvâ€
- â€œæŸ¥è¯¢å¯¹è±¡â€è‡ªåŠ¨æ¥è‡ªåŒç›®å½• â€œäººå‘˜ä¿¡æ¯.csvâ€ çš„â€œå®¢æˆ·å§“åâ€
- â€œåé¦ˆå•ä½â€æ¥è‡ª CSV æ–‡ä»¶çˆ¶æ–‡ä»¶å¤¹å

ï¼ˆ2025-08-28 é‡æ„äº®ç‚¹ï¼šåŠŸèƒ½ç­‰ä»·ã€æ€§èƒ½æ›´ä½³ï¼‰
- ç¼“å­˜ header æ¢æµ‹/ç‰¹æ®Šè¡¨å¤´æ¢æµ‹ï¼Œé¿å…å¤šæ¬¡è¯»å–åŒä¸€æ–‡ä»¶é¦–è¡Œ
- è§£ææ³°éš†å¤š sheet æ—¶åªè®¡ç®—ä¸€æ¬¡ header è¡Œ
- æ˜ŸæœŸ/èŠ‚å‡æ—¥åŸºäºå·²ç®—å¥½çš„æ—¶é—´æˆ³ä¸€æ¬¡æ€§å‘é‡åŒ–ç”Ÿæˆï¼ˆé”™è¯¯/NaT æ ‡æ³¨ä¿æŒä¸€è‡´ï¼‰
- å°½é‡å‡å°‘ DataFrame copy ä¸é‡å¤ç±»å‹è½¬æ¢
- I/O å°ä¼˜åŒ–ï¼ˆè‡ªåŠ¨åˆ—å®½é€»è¾‘ä¿ç•™ï¼Œåˆ†æ”¯æ›´ç²¾ç®€ï¼‰

ï¼ˆ2025-08-28+ èŒåŠ¡å¢å¼ºï¼‰
- è‡ªåŠ¨è¯»å–ç›®å½•å†…åŒ…å«â€œé€šè®¯å½•â€çš„Excelï¼ˆå¤šsheetï¼‰ï¼Œæå– å§“åã€ä¸»ç®¡éƒ¨é—¨åç§°ã€è¡Œæ”¿èŒåŠ¡
- â€œèŒåŠ¡â€= ä¸»ç®¡éƒ¨é—¨åç§°-è¡Œæ”¿èŒåŠ¡ï¼ˆç¼ºä¸€å–ä¸€ï¼‰
- ä¸äº¤æ˜“ä¸­çš„â€œäº¤æ˜“å¯¹æ–¹å§“åâ€åŒ¹é…ï¼Œè¾“å‡ºâ€œå¯¹æ–¹èŒåŠ¡â€åˆ°ï¼š
  1) æ‰€æœ‰äºº-åˆå¹¶äº¤æ˜“æµæ°´.xlsx
  2) èµ„é‡‘æ¥æºåˆ†æ
  3) äº¤æ˜“å¯¹æ‰‹åˆ†æ / ä¸å…¬å¸ç›¸å…³äº¤æ˜“é¢‘æ¬¡åˆ†æ

ï¼ˆ2025-09-09 çº¿ä¸‹é“¶è¡Œæ‰©å±•ï¼‰
- æ–°å¢æ¥å…¥ï¼šå†œä¸šé“¶è¡Œçº¿ä¸‹ï¼ˆè¯†åˆ« APSH sheetï¼‰
- æ–°å¢æ¥å…¥ï¼šå»ºè®¾é“¶è¡Œçº¿ä¸‹ï¼ˆè¯†åˆ« â€œäº¤æ˜“æ˜ç»†â€ sheetï¼‰

ï¼ˆ2025-09-09+ æ•°æ®è´¨é‡å¢å¼ºï¼‰
- ä¸‰é”®å»é‡ï¼šè‹¥ã€äº¤æ˜“æµæ°´å· + äº¤æ˜“æ—¶é—´ + äº¤æ˜“é‡‘é¢ã€‘å®Œå…¨ä¸€è‡´ï¼Œè‡ªåŠ¨å»é‡
"""

import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import threading, warnings, builtins, datetime, re
from pathlib import Path
from functools import wraps, lru_cache
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np
from chinese_calendar import is_holiday, is_workday

warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")

# ------------------------------------------------------------------
OUT_DIR: Optional[Path] = None
full_ts_pat = re.compile(r"\d{4}-\d{2}-\d{2}-\d{2}\.\d{2}\.\d{2}\.\d+")
TEMPLATE_COLS = [
    "åºå·","æŸ¥è¯¢å¯¹è±¡","åé¦ˆå•ä½","æŸ¥è¯¢é¡¹","æŸ¥è¯¢è´¦æˆ·","æŸ¥è¯¢å¡å·","äº¤æ˜“ç±»å‹","å€Ÿè´·æ ‡å¿—","å¸ç§",
    "äº¤æ˜“é‡‘é¢","è´¦æˆ·ä½™é¢","äº¤æ˜“æ—¶é—´","äº¤æ˜“æµæ°´å·","æœ¬æ–¹è´¦å·","æœ¬æ–¹å¡å·","äº¤æ˜“å¯¹æ–¹å§“å","äº¤æ˜“å¯¹æ–¹è´¦æˆ·",
    "äº¤æ˜“å¯¹æ–¹å¡å·","äº¤æ˜“å¯¹æ–¹è¯ä»¶å·ç ","äº¤æ˜“å¯¹æ‰‹ä½™é¢","äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ","äº¤æ˜“æ‘˜è¦","äº¤æ˜“ç½‘ç‚¹åç§°",
    "äº¤æ˜“ç½‘ç‚¹ä»£ç ","æ—¥å¿—å·","ä¼ ç¥¨å·","å‡­è¯ç§ç±»","å‡­è¯å·","ç°é‡‘æ ‡å¿—","ç»ˆç«¯å·","äº¤æ˜“æ˜¯å¦æˆåŠŸ",
    "äº¤æ˜“å‘ç”Ÿåœ°","å•†æˆ·åç§°","å•†æˆ·å·","IPåœ°å€","MAC","äº¤æ˜“æŸœå‘˜å·","å¤‡æ³¨",
]

# æ–°å¢ï¼šå…¨å±€é€šè®¯å½•â€œå§“å->èŒåŠ¡â€æ˜ å°„ï¼ˆä¾›åˆå¹¶åŠåˆ†æé˜¶æ®µä½¿ç”¨ï¼‰
CONTACT_TITLE_MAP: Dict[str, str] = {}

# ------------------------------------------------------------------
# â‘¡   é€šç”¨å·¥å…·
# ------------------------------------------------------------------
SKIP_HEADER_KEYWORDS = [
    "åæ´—é’±-ç”µå­è´¦æˆ·äº¤æ˜“æ˜ç»†",
    "ä¿¡ç”¨å¡æ¶ˆè´¹æ˜ç»†",
]

@lru_cache(maxsize=None)
def _should_skip_special_cached(path_str: str) -> Optional[str]:
    """é¦– 3 è¡ŒåŒ…å«å…³é”®å­—åˆ™è¿”å›å…³é”®å­—ï¼Œå¦åˆ™ Noneï¼›å¸¦ç¼“å­˜"""
    p = Path(path_str)
    try:
        head = pd.read_excel(p, header=None, nrows=3)
        for kw in SKIP_HEADER_KEYWORDS:
            if head.astype(str).apply(lambda col: col.astype(str).str.contains(kw, na=False)).any().any():
                return kw
        return None
    except Exception:
        return None

def should_skip_special(p: Path) -> Optional[str]:
    return _should_skip_special_cached(str(p))

def _normalize_time(t: str, is_old: bool) -> str:
    if not t:
        return ""
    if "." in t:
        t = re.sub(r"\.(\d{1,6})$", lambda m: ":" + m.group(1).ljust(6, "0"), t.replace(".", ":"))
    if re.fullmatch(r"\d{6}", t):
        t = f"{t[:2]}:{t[2:4]}:{t[4:]}"
    elif re.fullmatch(r"\d{4}", t):
        t = f"{t[:2]}:{t[2:]}:00"
    if is_old and re.fullmatch(r"\d{6}", t.replace(":", "")):
        t_num = t.replace(":", "")
        t = f"{t_num[:2]}:{t_num[2:4]}:{t_num[4:]}"
    return t

def save_df_auto_width(
    df: pd.DataFrame,
    filename: Path | str,
    sheet_name: str = "Sheet1",
    index: bool = False,
    engine: str = "xlsxwriter",
    min_width: int = 6,
    max_width: int = 50,
):
    if OUT_DIR is not None:
        filename = OUT_DIR / filename
    filename = Path(filename).with_suffix(".xlsx")
    filename.parent.mkdir(parents=True, exist_ok=True)

    df = df.replace(np.nan, "")
    if engine == "xlsxwriter":
        with pd.ExcelWriter(filename, engine="xlsxwriter") as writer:
            df.to_excel(writer, sheet_name=sheet_name, index=index)
            ws = writer.sheets[sheet_name]
            # è®¡ç®—åˆ—å®½ï¼ˆä¸€æ¬¡ map + maxï¼‰
            for i, col in enumerate(df.columns):
                s = df[col].astype(str)
                width = max(
                    min_width,
                    min(max(s.map(len).max(), len(str(col))) + 2, max_width),
                )
                ws.set_column(i, i, width)
    else:  # openpyxl
        df.to_excel(filename, sheet_name=sheet_name, index=index, engine="openpyxl")
        from openpyxl import load_workbook
        wb = load_workbook(filename)
        ws = wb[sheet_name]
        for col_cells in ws.columns:
            width = max(len(str(c.value)) if c.value is not None else 0 for c in col_cells) + 2
            ws.column_dimensions[col_cells[0].column_letter].width = max(
                min_width, min(width, max_width)
            ) + 5
        wb.save(filename)

def str_to_weekday(date_val) -> str:
    dt = pd.to_datetime(date_val, errors="coerce")
    return "wrong" if pd.isna(dt) else ["æ˜ŸæœŸä¸€","æ˜ŸæœŸäºŒ","æ˜ŸæœŸä¸‰","æ˜ŸæœŸå››","æ˜ŸæœŸäº”","æ˜ŸæœŸå…­","æ˜ŸæœŸæ—¥"][dt.weekday()]

def holiday_status(date_val) -> str:
    dt = pd.to_datetime(date_val, errors="coerce")
    if pd.isna(dt):
        return "wrong"
    d = dt.date()
    try:
        return "èŠ‚å‡æ—¥" if is_holiday(d) else ("å·¥ä½œæ—¥" if is_workday(d) else "å‘¨æœ«")
    except Exception:
        return "å‘¨æœ«" if dt.weekday() >= 5 else "å·¥ä½œæ—¥"

# ----------ï¼ˆæ–°å¢ï¼‰CSV/äººå‘˜ä¿¡æ¯ç›¸å…³é€šç”¨å‡½æ•° ----------
def _read_csv_smart(p: Path, **kwargs) -> pd.DataFrame:
    """æ™ºèƒ½ç¼–ç è¯»å– CSVï¼šä¼˜å…ˆ utf-8-sigï¼Œå…¶æ¬¡ gb18030ï¼Œå†é€€å› utf-8/cp936ã€‚"""
    enc_try = ["utf-8-sig", "gb18030", "utf-8", "cp936"]
    last_err: Optional[Exception] = None
    for enc in enc_try:
        try:
            return pd.read_csv(p, encoding=enc, **kwargs)
        except Exception as e:
            last_err = e
    raise last_err or RuntimeError(f"æ— æ³•è¯»å–CSV: {p}")

def _person_from_people_csv(dirpath: Path) -> str:
    """åŒç›®å½• â€˜äººå‘˜ä¿¡æ¯.csvâ€™ ä¸­ä¼˜å…ˆå–åˆ— â€˜å®¢æˆ·å§“åâ€™ çš„é¦–ä¸ªéç©ºå€¼ï¼›æä¾›ç¨³å¥å…œåº•ã€‚"""
    people = dirpath / "äººå‘˜ä¿¡æ¯.csv"
    if not people.exists():
        return ""
    try:
        df = _read_csv_smart(people)
    except Exception:
        return ""
    # ç›´æ¥åˆ—åå‘½ä¸­
    for col in ["å®¢æˆ·å§“å", "å§“å", "å®¢æˆ·åç§°", "æˆ·å"]:
        if col in df.columns:
            ser = df[col].astype(str).str.strip()
            ser = ser[(ser != "") & (ser.str.lower() != "nan")]
            if not ser.empty:
                return ser.iloc[0][:10]
    # è¡¨æ ¼é‡Œæ··å†™çš„â€œå®¢æˆ·å§“å:å¼ ä¸‰â€
    name_pat = re.compile(r"å®¢æˆ·(?:å§“å|åç§°)\s*[:ï¼š]?\s*([^\s:ï¼š]{2,10})")
    vals = df.astype(str).replace("nan", "", regex=False).to_numpy().ravel().tolist()
    for val in vals:
        m = name_pat.search(val.strip())
        if m:
            return m.group(1)
    return ""

# ------------------------------------------------------------------
# â‘¢   äººåè¯†åˆ«è¾…åŠ©
# ------------------------------------------------------------------
NAME_CANDIDATE_COLS: List[str] = [
    "è´¦æˆ·åç§°", "æˆ·å", "è´¦æˆ·å", "è´¦å·åç§°", "è´¦å·å", "å§“å", "å®¢æˆ·åç§°", "æŸ¥è¯¢å¯¹è±¡"
]

def extract_holder_from_df(raw: pd.DataFrame) -> str:
    for col in raw.columns:
        if any(key in col for key in NAME_CANDIDATE_COLS):
            s = raw[col].dropna()
            if not s.empty:
                v = str(s.iloc[0]).strip()
                if v and len(v) <= 10:
                    return v
    return ""

def fallback_holder_from_path(p: Path) -> str:
    name = p.parent.name
    if "å†œå•†è¡Œ" in name:
        name = p.parent.parent.name if p.parent.parent != p.parent else ""
    if not name or "å†œå•†è¡Œ" in name:
        name = re.split(r"[-_]", p.stem)[0]
    return name or "æœªçŸ¥"

@lru_cache(maxsize=None)
def holder_from_folder(folder: Path) -> str:
    for fp in folder.glob("*.xls*"):
        try:
            header_idx = _header_row(fp)          # è‡ªåŠ¨å®šä½è¡¨å¤´è¡Œï¼ˆç¼“å­˜ååªç®—ä¸€æ¬¡ï¼‰
            preview = pd.read_excel(fp, header=header_idx, nrows=5)
            if "è´¦æˆ·åç§°" in preview.columns:
                s = preview["è´¦æˆ·åç§°"].dropna()
                if not s.empty:
                    return str(s.iloc[0]).strip()
        except Exception:
            pass
    return ""

# ------------------------------------------------------------------
# â‘£   è§£æå‡½æ•°
# ------------------------------------------------------------------
@lru_cache(maxsize=None)
def _header_row(path: Path) -> int:
    """è¯»å–æ–‡ä»¶é¦– 15 è¡Œå¯»æ‰¾åŒ…å«â€œäº¤æ˜“æ—¥æœŸâ€çš„è¡Œå·ï¼›å¸¦ç¼“å­˜"""
    raw = pd.read_excel(path, header=None, nrows=15)
    for i, r in raw.iterrows():
        if "äº¤æ˜“æ—¥æœŸ" in r.values:
            return i
    return 0

def _read_raw(p: Path) -> pd.DataFrame:
    try:
        return pd.read_excel(p, header=_header_row(p))
    except Exception as e:
        print("âŒ", p.name, e)
        return pd.DataFrame()

def _parse_dt(d, t, is_old):
    try:
        if isinstance(t, str) and full_ts_pat.fullmatch(t.strip()):
            dt = pd.to_datetime(t, format="%Y-%m-%d-%H.%M.%S.%f", errors="coerce")
        else:
            dt = pd.to_datetime(f"{d} {_normalize_time(str(t), is_old)}".strip(), errors="coerce")
        return dt.strftime("%Y-%m-%d %H:%M:%S") if pd.notna(dt) else "wrong"
    except Exception:
        return "wrong"

# ----------ï¼ˆä¿®å¤&å¢å¼ºï¼‰äº¤æ˜“æ˜ç»† CSV â†’ æ¨¡æ¿ ----------
def csv_to_template(raw: pd.DataFrame, holder: str, feedback_unit: str) -> pd.DataFrame:
    """
    å°†â€˜äº¤æ˜“æ˜ç»†ä¿¡æ¯.csvâ€™æ˜ å°„æˆç»Ÿä¸€æ¨¡æ¿ï¼›å­—æ®µå°½é‡å¯¹é½ï¼Œä¸å¼ºä¾èµ–å›ºå®šè¡¨å¤´ã€‚
    - æŸ¥è¯¢å¯¹è±¡ï¼šä¼ å…¥ holder
    - åé¦ˆå•ä½ï¼šä¼ å…¥ feedback_unitï¼ˆçˆ¶ç›®å½•åï¼‰
    - è‹¥ä»»ä¸€å­—æ®µè½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™ï¼šè¯¥å­—æ®µæ•´åˆ—å¡«å…¥ 'wrong' å¹¶ç»§ç»­
    """
    if raw.empty:
        return pd.DataFrame(columns=TEMPLATE_COLS)

    try:
        df = raw.copy()
        df.columns = pd.Index(df.columns).astype(str).str.strip()
        n = len(df)

        def _S(default=""):
            return pd.Series([default] * n, index=df.index)

        def _safe(name, fn):
            try:
                return fn()
            except Exception as e:
                print(f"âš ï¸ CSVå­—æ®µ[{name}]è§£æå¼‚å¸¸ï¼š{e}")
                return _S("wrong")

        def col(keys, default=""):
            if isinstance(keys, str):
                return df[keys] if keys in df else _S(default)
            for k in keys:
                if k in df:
                    return df[k]
            return _S(default)

        def _to_str_no_sci(x):
            if pd.isna(x):
                return ""
            s = str(x).strip()
            if s.lower() == "nan":
                return ""
            if re.fullmatch(r"\d+\.0", s):
                return s[:-2]
            try:
                if isinstance(x, (int, np.integer)):
                    return str(int(x))
                if isinstance(x, float):
                    return f"{x:.0f}" if np.isfinite(x) else ""
                if re.fullmatch(r"\d+(\.\d+)?[eE][+-]?\d+", s):
                    return f"{float(s):.0f}"
            except Exception:
                pass
            return s

        def _std_success(v):
            s = str(v).strip()
            if s in {"1","Y","y","æ˜¯","æˆåŠŸ","True","true"}: return "æˆåŠŸ"
            if s in {"0","N","n","å¦","å¤±è´¥","False","false"}: return "å¤±è´¥"
            return "" if s.lower() == "nan" else s

        out = pd.DataFrame(index=df.index)

        # ===== æœ¬æ–¹è´¦å·/å¡å· + æŸ¥è¯¢è´¦æˆ·/å¡å· =====
        out["æœ¬æ–¹è´¦å·"] = _safe("æœ¬æ–¹è´¦å·", lambda: col(["äº¤æ˜“è´¦å·","æŸ¥è¯¢è´¦æˆ·","æœ¬æ–¹è´¦å·","è´¦å·","è´¦å·/å¡å·","è´¦å·å¡å·"]).map(_to_str_no_sci))
        out["æœ¬æ–¹å¡å·"] = _safe("æœ¬æ–¹å¡å·", lambda: col(["äº¤æ˜“å¡å·","æŸ¥è¯¢å¡å·","æœ¬æ–¹å¡å·","å¡å·"]).map(_to_str_no_sci))
        out["æŸ¥è¯¢è´¦æˆ·"] = out["æœ¬æ–¹è´¦å·"]
        out["æŸ¥è¯¢å¡å·"] = out["æœ¬æ–¹å¡å·"]

        # ===== å¯¹æ–¹è´¦å·/å¡å·ï¼ˆæŒ‰ ç±»å‹ åˆ—åˆ†æµï¼‰=====
        opp_no  = _safe("äº¤æ˜“å¯¹æ‰‹è´¦å¡å·", lambda: col(["äº¤æ˜“å¯¹æ‰‹è´¦å¡å·","äº¤æ˜“å¯¹æ‰‹è´¦å·","å¯¹æ–¹è´¦å·","å¯¹æ–¹è´¦æˆ·"]).map(_to_str_no_sci))
        opp_typ = col(["äº¤æ˜“å¯¹æ–¹å¸å¡å·ç±»å‹","è´¦å·/å¡å·ç±»å‹"], "")
        typ_s   = opp_typ.astype(str)
        is_card = _safe("äº¤æ˜“å¯¹æ–¹å¸å¡å·ç±»å‹", lambda: typ_s.str.contains("å¡", na=False) | typ_s.isin(["2","å¡","å¡å·"]))
        out["äº¤æ˜“å¯¹æ–¹å¡å·"] = np.where(is_card, opp_no, "")
        out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"] = np.where(is_card, "", opp_no)

        # ===== åŸºæœ¬ä¿¡æ¯ =====
        out["æŸ¥è¯¢å¯¹è±¡"] = holder or "æœªçŸ¥"
        out["åé¦ˆå•ä½"] = feedback_unit or "æœªçŸ¥"
        out["å¸ç§"] = _safe("å¸ç§", lambda: col(["äº¤æ˜“å¸ç§","å¸ç§","å¸åˆ«","è´§å¸"], "CNY").astype(str).replace(
            {"äººæ°‘å¸":"CNY","äººæ°‘å¸å…ƒ":"CNY","RMB":"CNY","156":"CNY"}).fillna("CNY"))

        # ===== é‡‘é¢ / ä½™é¢ =====
        out["äº¤æ˜“é‡‘é¢"] = _safe("äº¤æ˜“é‡‘é¢", lambda: pd.to_numeric(col(["äº¤æ˜“é‡‘é¢","é‡‘é¢","å‘ç”Ÿé¢"], 0), errors="coerce"))
        out["è´¦æˆ·ä½™é¢"] = _safe("è´¦æˆ·ä½™é¢", lambda: pd.to_numeric(col(["äº¤æ˜“ä½™é¢","ä½™é¢","è´¦æˆ·ä½™é¢"], 0), errors="coerce"))

        # ===== å€Ÿè´·/æ”¶ä»˜ =====
        out["å€Ÿè´·æ ‡å¿—"] = col(["æ”¶ä»˜æ ‡å¿—",""], "")

        # ===== äº¤æ˜“æ—¶é—´ =====
        if "äº¤æ˜“æ—¶é—´" in df.columns:
            out["äº¤æ˜“æ—¶é—´"] = _safe("äº¤æ˜“æ—¶é—´", lambda: np.where(
                pd.to_datetime(df["äº¤æ˜“æ—¶é—´"], errors="coerce").notna(),
                pd.to_datetime(df["äº¤æ˜“æ—¶é—´"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M:%S"),
                df["äº¤æ˜“æ—¶é—´"].astype(str)
            ))
        else:
            out["äº¤æ˜“æ—¶é—´"] = _S("wrong")

        # ===== å…¶å®ƒå­—æ®µå¯¹é½ =====
        out["äº¤æ˜“ç±»å‹"]              = col(["äº¤æ˜“ç±»å‹","ä¸šåŠ¡ç§ç±»","äº¤æ˜“ç "], "")
        out["äº¤æ˜“æµæ°´å·"]            = col(["äº¤æ˜“æµæ°´å·","æŸœå‘˜æµæ°´å·","æµæ°´å·"], "")
        out["äº¤æ˜“å¯¹æ–¹å§“å"]           = col(["å¯¹æ‰‹æˆ·å","äº¤æ˜“å¯¹æ‰‹åç§°","å¯¹æ‰‹æ–¹åç§°","å¯¹æ–¹æˆ·å","å¯¹æ–¹åç§°","å¯¹æ–¹å§“å","æ”¶/ä»˜æ–¹åç§°"], " ")
        out["äº¤æ˜“å¯¹æ–¹è¯ä»¶å·ç "]         = col(["å¯¹æ‰‹èº«ä»½è¯å·","å¯¹æ–¹è¯ä»¶å·ç "], " ")
        out["äº¤æ˜“å¯¹æ‰‹ä½™é¢"]           = _safe("äº¤æ˜“å¯¹æ‰‹ä½™é¢", lambda: pd.to_numeric(col(["å¯¹æ‰‹äº¤æ˜“ä½™é¢"], ""), errors="coerce"))
        out["äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ"]        = col(["å¯¹æ‰‹å¼€æˆ·é“¶è¡Œ","äº¤æ˜“å¯¹æ‰‹è¡Œå","å¯¹æ–¹å¼€æˆ·è¡Œ","å¯¹æ–¹é‡‘èæœºæ„åç§°"], " ")
        out["äº¤æ˜“æ‘˜è¦"]              = col(["æ‘˜è¦è¯´æ˜","äº¤æ˜“æ‘˜è¦","æ‘˜è¦","é™„è¨€","ç”¨é€”"], " ")
        out["äº¤æ˜“ç½‘ç‚¹åç§°"]            = col(["äº¤æ˜“ç½‘ç‚¹åç§°","äº¤æ˜“æœºæ„","ç½‘ç‚¹åç§°"], "")
        out["äº¤æ˜“ç½‘ç‚¹ä»£ç "]            = col(["äº¤æ˜“ç½‘ç‚¹ä»£ç ","æœºæ„å·","ç½‘ç‚¹ä»£ç "], "")
        out["æ—¥å¿—å·"]               = col(["æ—¥å¿—å·"], "")
        out["ä¼ ç¥¨å·"]               = col(["ä¼ ç¥¨å·"], "")
        out["å‡­è¯ç§ç±»"]              = col(["å‡­è¯ç§ç±»","å‡­è¯ç±»å‹"], "")
        out["å‡­è¯å·"]               = col(["å‡­è¯å·","å‡­è¯åºå·"], "")
        out["ç°é‡‘æ ‡å¿—"]              = col(["ç°é‡‘æ ‡å¿—"], "")
        out["ç»ˆç«¯å·"]               = col(["ç»ˆç«¯å·","æ¸ é“å·"], "")
        succ                        = col(["äº¤æ˜“æ˜¯å¦æˆåŠŸ","æŸ¥è¯¢åé¦ˆç»“æœ"], "")
        out["äº¤æ˜“æ˜¯å¦æˆåŠŸ"]            = succ.map(_std_success)
        out["äº¤æ˜“å‘ç”Ÿåœ°"]             = col(["äº¤æ˜“å‘ç”Ÿåœ°","äº¤æ˜“åœºæ‰€"], "")
        out["å•†æˆ·åç§°"]              = col(["å•†æˆ·åç§°"], "")
        out["å•†æˆ·å·"]               = col(["å•†æˆ·å·"], "")
        out["IPåœ°å€"]              = col(["IPåœ°å€"], "")
        out["MAC"]                = col(["MACåœ°å€","MAC"], "")
        out["äº¤æ˜“æŸœå‘˜å·"]             = col(["äº¤æ˜“æŸœå‘˜å·","æŸœå‘˜å·","è®°è´¦æŸœå‘˜"], "")

        # ===== å¤‡æ³¨åˆå¹¶ =====
        try:
            beizhu = col(["å¤‡æ³¨","é™„è¨€","è¯´æ˜"], "").astype(str)
            reason = col(["æŸ¥è¯¢åé¦ˆç»“æœåŸå› "], "").astype(str)
            beizhu_clean = beizhu.where(~beizhu.str.lower().eq("nan"), "")
            reason_clean = reason.where(~reason.str.lower().eq("nan"), "")
            out["å¤‡æ³¨"] = np.where(
                reason_clean != "",
                np.where(beizhu_clean != "", beizhu_clean + "ï½œåŸå› ï¼š" + reason_clean, "åŸå› ï¼š" + reason_clean),
                beizhu_clean,
            )
        except Exception as e:
            print(f"âš ï¸ CSVå­—æ®µ[å¤‡æ³¨/åŸå› ]è§£æå¼‚å¸¸ï¼š{e}")
            out["å¤‡æ³¨"] = _S("wrong")

        # å¯¹é½æ¨¡æ¿åˆ—é¡ºåº
        out = out.reindex(columns=TEMPLATE_COLS, fill_value="")
        return out

    except Exception as e:
        print(f"âŒ CSVè½¬æ¨¡æ¿å‘ç”Ÿå¼‚å¸¸ï¼š{e}")
        n = len(raw)
        bad = pd.DataFrame({col: ["wrong"] * n for col in TEMPLATE_COLS})
        return bad

# ===============================
# â‘¤   æ³°éš†é“¶è¡Œ â†’ æ¨¡æ¿
# ===============================
def tl_to_template(raw) -> pd.DataFrame:
    """
    æ³°éš†é“¶è¡Œæµæ°´ â†’ ç»Ÿä¸€æ¨¡æ¿å­—æ®µ TEMPLATE_COLS
    2025-08-06  å¢å¼ºç‰ˆï¼šæ”¯æŒ dict[sheet, df] åˆå¹¶ï¼›ä¿æŒåŸè¡Œä¸º
    """
    if isinstance(raw, dict):
        frames: List[pd.DataFrame] = []
        for sheet_name, df_sheet in raw.items():
            one = tl_to_template(df_sheet)
            if not one.empty:
                one.insert(0, "__sheet__", sheet_name)
                frames.append(one)
        return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=TEMPLATE_COLS)

    df: pd.DataFrame = raw
    if df.empty:
        return pd.DataFrame(columns=TEMPLATE_COLS)

    def col_multi(keys, default=""):
        for k in keys:
            if k in df:
                return df[k]
        return pd.Series([default] * len(df), index=df.index)

    out = pd.DataFrame(index=df.index)
    out["æœ¬æ–¹è´¦å·"] = col_multi(["å®¢æˆ·è´¦å·","è´¦å·","æœ¬æ–¹è´¦å·"], "wrong")
    out["æŸ¥è¯¢è´¦æˆ·"] = out["æœ¬æ–¹è´¦å·"]
    out["åé¦ˆå•ä½"] = "æ³°éš†é“¶è¡Œ"
    out["æŸ¥è¯¢å¯¹è±¡"] = col_multi(["è´¦æˆ·åç§°","æˆ·å","å®¢æˆ·åç§°"], "wrong")
    out["å¸ç§"] = col_multi(["å¸ç§","è´§å¸","å¸åˆ«"]).replace("156","CNY").replace("äººæ°‘å¸å…ƒ","CNY").replace("äººæ°‘å¸","CNY").fillna("CNY")
    out["å€Ÿè´·æ ‡å¿—"] = col_multi(["å€Ÿè´·æ ‡å¿—","å€Ÿè´·æ–¹å‘","å€Ÿè´·"], "")

    debit  = pd.to_numeric(col_multi(["å€Ÿæ–¹å‘ç”Ÿé¢","å€Ÿæ–¹å‘ç”Ÿé‡‘é¢"], 0), errors="coerce")
    credit = pd.to_numeric(col_multi(["è´·æ–¹å‘ç”Ÿé¢","è´·æ–¹å‘ç”Ÿé‡‘é¢"], 0), errors="coerce")
    out["äº¤æ˜“é‡‘é¢"] = debit.fillna(0).where(debit.ne(0), credit)
    out["è´¦æˆ·ä½™é¢"] = pd.to_numeric(col_multi(["è´¦æˆ·ä½™é¢","ä½™é¢"], 0), errors="coerce")

    dates = col_multi(["äº¤æ˜“æ—¥æœŸ","åŸäº¤æ˜“æ—¥æœŸ","ä¼šè®¡æ—¥æœŸ"]).astype(str)
    raw_times = col_multi(["äº¤æ˜“æ—¶é—´","åŸäº¤æ˜“æ—¶é—´","æ—¶é—´"]).astype(str).str.strip()

    def _tidy_time(s: str) -> str:
        if re.fullmatch(r"0+(\.0+)?", s):
            return ""
        if s.count(".") >= 2:
            p = s.split(".")
            if len(p[0]) == 2 and len(p[1]) == 2 and len(p[2]) == 2:
                return ".".join(p[:3])
        return s

    def _clean_time(s: str) -> str:
        s = s.strip()
        if re.fullmatch(r"0+(\.0+)?", s):
            return ""
        if re.fullmatch(r"\d{1,9}", s):
            return s.zfill(9)[:6]
        return s

    times = raw_times.apply(lambda x: _clean_time(_tidy_time(x)))
    out["äº¤æ˜“æ—¶é—´"] = [_parse_dt(d, t, is_old=False) for d, t in zip(dates, times)]

    out["äº¤æ˜“æµæ°´å·"]        = col_multi(["åŸæŸœå‘˜æµæ°´å·","æµæ°´å·"])
    out["äº¤æ˜“ç±»å‹"]          = col_multi(["äº¤æ˜“ç ","äº¤æ˜“ç±»å‹","ä¸šåŠ¡ç§ç±»"])
    out["äº¤æ˜“å¯¹æ–¹å§“å"]       = col_multi(["å¯¹æ–¹æˆ·å","äº¤æ˜“å¯¹æ‰‹åç§°"], " ")
    out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"]       = col_multi(["å¯¹æ–¹å®¢æˆ·è´¦å·","å¯¹æ–¹è´¦å·"], " ")
    out["äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ"]   = col_multi(["å¯¹æ–¹é‡‘èæœºæ„åç§°","å¯¹æ–¹å¼€æˆ·è¡Œ"], " ")
    out["äº¤æ˜“æ‘˜è¦"]          = col_multi(["æ‘˜è¦æè¿°","æ‘˜è¦"], " ")
    out["äº¤æ˜“ç½‘ç‚¹ä»£ç "]        = col_multi(["æœºæ„å·","ç½‘ç‚¹ä»£ç "], " ")
    out["ç»ˆç«¯å·"]           = col_multi(["æ¸ é“å·","ç»ˆç«¯å·"], " ")
    out["äº¤æ˜“æŸœå‘˜å·"]         = col_multi(["æŸœå‘˜å·"], " ")
    out["å¤‡æ³¨"]            = col_multi(["å¤‡æ³¨","é™„è¨€"], " ")

    out["å‡­è¯ç§ç±»"] = col_multi(["å‡­è¯ç±»å‹"], "")
    out["å‡­è¯å·"]   = col_multi(["å‡­è¯åºå·"], "")

    out = out.reindex(columns=TEMPLATE_COLS, fill_value="")
    return out

# ------------------------------------------------------------------
# â‘¤   æ°‘æ³° â†’ æ¨¡æ¿
# ------------------------------------------------------------------
def mt_to_template(raw: pd.DataFrame) -> pd.DataFrame:
    if raw.empty:
        return pd.DataFrame(columns=TEMPLATE_COLS)

    header_idx = None
    for i, row in raw.iterrows():
        cells = row.astype(str).str.strip().tolist()
        if "æ—¶é—´" in cells and "è´¦å·å¡å·" in cells:
            header_idx = i
            break
    if header_idx is None:
        for i, row in raw.iterrows():
            if row.astype(str).str.contains("åºå·").any():
                header_idx = i
                break
    if header_idx is None:
        return pd.DataFrame(columns=TEMPLATE_COLS)

    holder = ""
    name_inline = re.compile(r"å®¢æˆ·(?:å§“å|åç§°)\s*[:ï¼š]?\s*([^\s:ï¼š]{2,10})")
    for i in range(header_idx):
        vals = raw.iloc[i].astype(str).tolist()
        for j, cell in enumerate(vals):
            cs = cell.strip()
            m = name_inline.match(cs)
            if m:
                holder = m.group(1)
                break
            if re.fullmatch(r"å®¢æˆ·(?:å§“å|åç§°)\s*[:ï¼š]?", cs):
                nxt = str(vals[j+1]).strip() if j+1 < len(vals) else ""
                if nxt and nxt.lower() != "nan":
                    holder = nxt
                    break
        if holder:
            break
    holder = holder or "æœªçŸ¥"

    df = raw.iloc[header_idx + 1:].copy()
    df.columns = raw.iloc[header_idx].astype(str).str.strip()
    df.dropna(how="all", inplace=True)
    df.reset_index(drop=True, inplace=True)

    summary_mask = df.apply(
        lambda row: row.astype(str).str.contains(r"æ”¯å‡ºç¬”æ•°|æ”¶å…¥ç¬”æ•°|æ”¯å‡ºç´¯è®¡é‡‘é¢|æ”¶å…¥ç´¯è®¡é‡‘é¢").any(),
        axis=1,
    )
    df = df[~summary_mask].copy()

    def col(c, default=""):
        return df[c] if c in df else pd.Series(default, index=df.index)

    out = pd.DataFrame(index=df.index)
    acct = col("è´¦å·å¡å·").astype(str).str.replace(r"\.0$", "", regex=True)
    out["æœ¬æ–¹è´¦å·"] = acct
    out["æŸ¥è¯¢è´¦æˆ·"] = acct
    out["æŸ¥è¯¢å¯¹è±¡"] = holder
    out["åé¦ˆå•ä½"] = "æ°‘æ³°é“¶è¡Œ"
    out["å¸ç§"] = col("å¸ç§").astype(str).replace("äººæ°‘å¸","CNY").replace("äººæ°‘å¸å…ƒ","CNY").fillna("CNY")

    debit  = pd.to_numeric(col("æ”¯å‡º"), errors="coerce").fillna(0)
    credit = pd.to_numeric(col("æ”¶å…¥"), errors="coerce").fillna(0)
    out["äº¤æ˜“é‡‘é¢"] = credit.where(credit.gt(0), -debit)
    out["è´¦æˆ·ä½™é¢"] = pd.to_numeric(col("ä½™é¢"), errors="coerce")
    out["å€Ÿè´·æ ‡å¿—"] = np.where(credit.gt(0), "è¿›", "å‡º")

    def _fmt_time(v: str) -> str:
        v = str(v).strip()
        try:
            return datetime.datetime.strptime(v, "%Y%m%d %H:%M:%S").strftime("%Y-%m-%d %H:%M:%S")
        except Exception:
            return v or "wrong"

    out["äº¤æ˜“æ—¶é—´"] = col("æ—¶é—´").astype(str).apply(_fmt_time)

    out["äº¤æ˜“æ‘˜è¦"]        = col("æ‘˜è¦", " ")
    out["äº¤æ˜“æµæ°´å·"]      = col("æŸœå‘˜æµæ°´å·").astype(str).str.strip()
    out["äº¤æ˜“æŸœå‘˜å·"]       = col("è®°è´¦æŸœå‘˜ ").astype(str).str.strip()
    out["äº¤æ˜“ç½‘ç‚¹ä»£ç "]      = col("è®°è´¦æœºæ„").astype(str).str.strip()
    out["äº¤æ˜“å¯¹æ–¹å§“å"]     = col("äº¤æ˜“å¯¹æ‰‹åç§°", " ").astype(str).str.strip()
    out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"]     = col("äº¤æ˜“å¯¹æ‰‹è´¦å·", " ").astype(str).str.strip()
    out["äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ"] = col("äº¤æ˜“å¯¹æ‰‹è¡Œå", " ").astype(str).str.strip()
    out["ç»ˆç«¯å·"]         = col("äº¤æ˜“æ¸ é“")
    out["å¤‡æ³¨"]          = col("é™„è¨€", " ")

    out = out.reindex(columns=TEMPLATE_COLS, fill_value="")
    return out

# ------------------------------------------------------------------
# â‘¤   å†œå•†è¡Œ â†’ æ¨¡æ¿
# ------------------------------------------------------------------
def rc_to_template(raw: pd.DataFrame, holder: str, is_old: bool) -> pd.DataFrame:
    if raw.empty:
        return pd.DataFrame(columns=TEMPLATE_COLS)

    def col(c, default=""):
        return raw[c] if c in raw else pd.Series([default] * len(raw), index=raw.index)

    out = pd.DataFrame(index=raw.index)
    out["æœ¬æ–¹è´¦å·"] = col("è´¦å·", "wrong")
    out["æŸ¥è¯¢è´¦æˆ·"] = out["æœ¬æ–¹è´¦å·"]
    out["äº¤æ˜“é‡‘é¢"] = col("å‘ç”Ÿé¢") if is_old else col("äº¤æ˜“é‡‘é¢")
    out["è´¦æˆ·ä½™é¢"] = col("ä½™é¢") if is_old else col("äº¤æ˜“ä½™é¢")
    out["åé¦ˆå•ä½"] = "è€å†œå•†é“¶è¡Œ" if is_old else "æ–°å†œå•†é“¶è¡Œ"

    dates = col("äº¤æ˜“æ—¥æœŸ").astype(str)
    times = col("äº¤æ˜“æ—¶é—´").astype(str)
    out["äº¤æ˜“æ—¶é—´"] = [_parse_dt(d, t, is_old) for d, t in zip(dates, times)]

    out["å€Ÿè´·æ ‡å¿—"] = col("å€Ÿè´·æ ‡å¿—")
    out["å¸ç§"] = "CNY" if is_old else col("å¸ç§").replace("äººæ°‘å¸","CNY").replace("äººæ°‘å¸å…ƒ","CNY")
    out["æŸ¥è¯¢å¯¹è±¡"] = holder
    out["äº¤æ˜“å¯¹æ–¹å§“å"] = col("å¯¹æ–¹å§“å", " ")
    out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"] = col("å¯¹æ–¹è´¦å·", " ")
    out["äº¤æ˜“ç½‘ç‚¹åç§°"] = col("ä»£ç†è¡Œæœºæ„å·") if is_old else col("äº¤æ˜“æœºæ„")
    out["äº¤æ˜“æ‘˜è¦"] = col("å¤‡æ³¨") if is_old else col("æ‘˜è¦", "wrong")

    out = out.reindex(columns=TEMPLATE_COLS, fill_value="")
    return out

# ===============================
# â‘¤.8  å†œä¸šé“¶è¡Œçº¿ä¸‹ï¼ˆAPSHï¼‰ â†’ æ¨¡æ¿ï¼ˆåˆå¹¶ yyyymmdd + HHMMSSï¼‰
# ===============================
def _is_abc_offline_file(p: Path) -> bool:
    """æ˜¯å¦ä¸ºå†œè¡Œçº¿ä¸‹æŸ¥è¯¢æ ¼å¼ï¼šå« APSH sheetã€‚"""
    try:
        xls = pd.ExcelFile(p)
        return "APSH" in xls.sheet_names
    except Exception:
        return False

def _merge_abc_datetime(date_val, time_val) -> str:
    """
    å°† yyyymmdd ä¸ æ—¶é—´(æ— è¿æ¥ç¬¦ HHMMSSï¼Œæˆ– 13:31:20ï¼Œæˆ– Excel å°æ•°æ—¶é—´ï¼Œæˆ–ç©º) åˆå¹¶ä¸º 'YYYY-MM-DD HH:MM:SS'ã€‚
    è§„åˆ™ï¼š
      - äº¤æ˜“æ—¶é—´ä¸ºç©º/NaN/ç©ºå­—ç¬¦ä¸² => 00:00:00
      - äº¤æ˜“æ—¶é—´ä¸º Excel å°æ•°(0~1) => æŒ‰ä¸€å¤©çš„ç§’æ•°æ¢ç®—
      - çº¯æ•°å­—é•¿åº¦<6 å·¦è¡¥é›¶ï¼Œ>6 å–å‰ 6 ä½
      - ç¤ºä¾‹ï¼š20100113 + 133120 -> 2010-01-13 13:31:20
    """
    # ---- æ—¥æœŸå¤„ç† ----
    ds_raw = "" if date_val is None else str(date_val).strip()
    ds_digits = re.sub(r"\D", "", ds_raw)
    date_ts = None
    if len(ds_digits) >= 8:
        ds8 = ds_digits[:8]
        date_ts = pd.to_datetime(ds8, format="%Y%m%d", errors="coerce")
    else:
        # å…œåº•ï¼šç›´æ¥å°è¯•è§£æ
        date_ts = pd.to_datetime(date_val, errors="coerce")
    if pd.isna(date_ts):
        return "wrong"
    date_str = date_ts.strftime("%Y-%m-%d")

    # ---- æ—¶é—´å¤„ç†ï¼šç»Ÿä¸€å¾—åˆ° 'HHMMSS' çš„ 6 ä½å­—ç¬¦ä¸² ----
    def to_hhmmss_str(t) -> str:
        # ç©ºã€NaNã€None -> 00:00:00
        if t is None or (isinstance(t, float) and np.isnan(t)) or (isinstance(t, str) and t.strip() == "") or pd.isna(t):
            return "000000"

        # Excel å°æ•°æ—¶é—´ï¼ˆ0~1ï¼‰
        if isinstance(t, (int, np.integer)) or isinstance(t, (float, np.floating)):
            try:
                tf = float(t)
                if 0.0 <= tf < 1.0:
                    secs = int(round(tf * 86400))
                    if secs >= 86400:
                        secs = 0  # æç«¯å››èˆäº”å…¥åˆ° 24:00:00ï¼Œå½“ä½œ 00:00:00
                    h = secs // 3600
                    m = (secs % 3600) // 60
                    s = secs % 60
                    return f"{h:02d}{m:02d}{s:02d}"
                # å¸¸è§ï¼š133120.0 / 93120.0
                digits = re.sub(r"\D", "", str(int(round(tf))))
                if len(digits) < 6:
                    digits = digits.zfill(6)
                else:
                    digits = digits[:6]
                return digits
            except Exception:
                pass

        # å­—ç¬¦ä¸²ï¼šå¯èƒ½æ˜¯ '13:31:20' / '13.31.20' / '133120' / '93120'
        s = str(t).strip()
        # å¸¦åˆ†éš”ç¬¦çš„æƒ…å†µï¼Œå°è¯•æŒ‰æ—¶é—´è§£æ
        if ":" in s or "." in s:
            s2 = s.replace(".", ":")
            tt = pd.to_datetime("2000-01-01 " + s2, errors="coerce")
            if pd.notna(tt):
                return tt.strftime("%H%M%S")
        # çº¯æå–æ•°å­—
        digits = re.sub(r"\D", "", s)
        if digits == "":
            return "000000"
        if len(digits) < 6:
            digits = digits.zfill(6)
        else:
            digits = digits[:6]
        return digits

    hhmmss = to_hhmmss_str(time_val)
    hh, mm, ss = hhmmss[:2], hhmmss[2:4], hhmmss[4:6]
    return f"{date_str} {hh}:{mm}:{ss}"

def abc_offline_from_file(p: Path) -> pd.DataFrame:
    """
    å†œä¸šé“¶è¡Œçº¿ä¸‹æŸ¥è¯¢ï¼ˆAPSHï¼‰æµæ°´ â†’ ç»Ÿä¸€æ¨¡æ¿å­—æ®µ TEMPLATE_COLS
    é€‚é…åˆ—ï¼ˆå¸¸è§ï¼‰ï¼šè´¦å·ã€äº¤æ˜“æ—¥æœŸ(yyyymmdd)ã€äº¤æ˜“æ—¶é—´(HHMMSSï¼Œæ— è¿æ¥ç¬¦)ã€å¡å·ã€æˆ·åã€ä¼ ç¥¨å·ã€äº¤æ˜“ç½‘ç‚¹ã€äº¤æ˜“é‡‘é¢ã€äº¤æ˜“åä½™é¢ã€
             æ‘˜è¦ã€äº¤æ˜“æ¸ é“ã€å¯¹æ–¹è´¦å·ã€å¯¹æ–¹æˆ·åã€å¯¹æ–¹å¼€æˆ·è¡Œã€äº¤æ˜“è¡Œå·
    â€”â€” æœ¬å‡½æ•°å°†ã€äº¤æ˜“æ—¥æœŸ + äº¤æ˜“æ—¶é—´ã€‘åˆå¹¶ç”Ÿæˆæ ‡å‡†â€œäº¤æ˜“æ—¶é—´(YYYY-MM-DD HH:MM:SS)â€ã€‚
    """
    try:
        xls = pd.ExcelFile(p)
        if "APSH" not in xls.sheet_names:
            return pd.DataFrame(columns=TEMPLATE_COLS)
        df = xls.parse("APSH", header=0)
    except Exception:
        return pd.DataFrame(columns=TEMPLATE_COLS)

    if df.empty:
        return pd.DataFrame(columns=TEMPLATE_COLS)

    # åˆ—åæ¸…æ´—
    df.columns = pd.Index(df.columns).astype(str).str.strip()
    n = len(df)
    out = pd.DataFrame(index=df.index)

    # æœ¬æ–¹/æŸ¥è¯¢è´¦å·å¡å·
    out["æœ¬æ–¹è´¦å·"] = df.get("è´¦å·", "")
    out["æœ¬æ–¹å¡å·"] = df.get("å¡å·", "").astype(str).str.replace(r"\.0$", "", regex=True)
    out["æŸ¥è¯¢è´¦æˆ·"] = out["æœ¬æ–¹è´¦å·"]
    out["æŸ¥è¯¢å¡å·"] = out["æœ¬æ–¹å¡å·"]

    # æŸ¥è¯¢å¯¹è±¡/åé¦ˆå•ä½/å¸ç§
    holder = df.get("æˆ·å", "")
    if not isinstance(holder, pd.Series):
        holder = pd.Series([holder]*n, index=df.index)
    out["æŸ¥è¯¢å¯¹è±¡"] = holder.fillna("").astype(str).str.strip().replace({"nan": ""}).replace("", "æœªçŸ¥")
    out["åé¦ˆå•ä½"] = "å†œä¸šé“¶è¡Œ"
    out["å¸ç§"] = "CNY"

    # é‡‘é¢/ä½™é¢/å€Ÿè´·æ ‡å¿—ï¼ˆæŒ‰æ­£è´Ÿå·åˆ¤æ–­ï¼‰
    amt = pd.to_numeric(df.get("äº¤æ˜“é‡‘é¢", 0), errors="coerce")
    out["äº¤æ˜“é‡‘é¢"] = amt
    out["è´¦æˆ·ä½™é¢"] = pd.to_numeric(df.get("äº¤æ˜“åä½™é¢", ""), errors="coerce")
    out["å€Ÿè´·æ ‡å¿—"] = np.where(amt > 0, "è¿›", np.where(amt < 0, "å‡º", ""))

    # === äº¤æ˜“æ—¶é—´ï¼šåˆå¹¶ yyyymmdd + HHMMSSï¼ˆæ— è¿æ¥ç¬¦ï¼‰ ===
    dates = df.get("äº¤æ˜“æ—¥æœŸ", "")
    times = df.get("äº¤æ˜“æ—¶é—´", "")
    out["äº¤æ˜“æ—¶é—´"] = [_merge_abc_datetime(d, t) for d, t in zip(dates, times)]

    # å…¶å®ƒå­—æ®µå¯¹é½
    out["äº¤æ˜“æ‘˜è¦"] = df.get("æ‘˜è¦", "").astype(str)
    out["äº¤æ˜“æµæ°´å·"] = ""  # APSH å¤šæ— æ­¤å­—æ®µ
    out["äº¤æ˜“ç±»å‹"] = ""    # å¯æ ¹æ®éœ€è¦ç”± æ‘˜è¦/æ¸ é“ æ¨æ–­ï¼›æ­¤å¤„ç•™ç©º
    out["äº¤æ˜“å¯¹æ–¹å§“å"] = df.get("å¯¹æ–¹æˆ·å", " ").astype(str)
    out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"] = df.get("å¯¹æ–¹è´¦å·", " ").astype(str)
    out["äº¤æ˜“å¯¹æ–¹å¡å·"] = ""
    out["äº¤æ˜“å¯¹æ–¹è¯ä»¶å·ç "] = " "
    out["äº¤æ˜“å¯¹æ‰‹ä½™é¢"] = ""
    out["äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ"] = df.get("å¯¹æ–¹å¼€æˆ·è¡Œ", " ").astype(str)
    out["äº¤æ˜“ç½‘ç‚¹åç§°"] = df.get("äº¤æ˜“ç½‘ç‚¹", "").astype(str)
    out["äº¤æ˜“ç½‘ç‚¹ä»£ç "] = df.get("äº¤æ˜“è¡Œå·", "").astype(str)
    out["æ—¥å¿—å·"] = ""
    out["ä¼ ç¥¨å·"] = df.get("ä¼ ç¥¨å·", "").astype(str)
    out["å‡­è¯ç§ç±»"] = ""
    out["å‡­è¯å·"] = ""
    out["ç°é‡‘æ ‡å¿—"] = ""
    out["ç»ˆç«¯å·"] = df.get("äº¤æ˜“æ¸ é“", "").astype(str)
    out["äº¤æ˜“æ˜¯å¦æˆåŠŸ"] = ""
    out["äº¤æ˜“å‘ç”Ÿåœ°"] = ""
    out["å•†æˆ·åç§°"] = ""
    out["å•†æˆ·å·"] = ""
    out["IPåœ°å€"] = ""
    out["MAC"] = ""
    out["äº¤æ˜“æŸœå‘˜å·"] = ""
    out["å¤‡æ³¨"] = ""

    # æ¨¡æ¿åˆ—é¡ºåº
    out = out.reindex(columns=TEMPLATE_COLS, fill_value="")
    return out

# ===============================
# â‘¤.9  å»ºè®¾é“¶è¡Œçº¿ä¸‹ï¼ˆäº¤æ˜“æ˜ç»†ï¼‰ â†’ æ¨¡æ¿ï¼ˆæ–°å¢ï¼‰
# ===============================
def _is_ccb_offline_file(p: Path) -> bool:
    """
    ç²—è¯†åˆ«å»ºè®¾é“¶è¡Œçº¿ä¸‹ï¼šå­˜åœ¨åä¸ºâ€œäº¤æ˜“æ˜ç»†â€çš„sheetï¼Œä¸”åŒ…å«å…³é”®å­—æ®µã€‚
    """
    try:
        xls = pd.ExcelFile(p)
        if "äº¤æ˜“æ˜ç»†" not in xls.sheet_names:
            return False
        # å–å¤´ä¸€è¡Œçœ‹åˆ—åæ˜¯å¦å«å…³é”®å­—æ®µ
        df_head = xls.parse("äº¤æ˜“æ˜ç»†", nrows=1)
        cols = set(map(str, df_head.columns))
        required = {"å®¢æˆ·åç§°", "è´¦å·", "äº¤æ˜“æ—¥æœŸ", "äº¤æ˜“æ—¶é—´", "äº¤æ˜“é‡‘é¢"}
        return required.issubset(cols)
    except Exception:
        return False

def ccb_offline_from_file(p: Path) -> pd.DataFrame:
    """
    å»ºè®¾é“¶è¡Œçº¿ä¸‹ï¼ˆäº¤æ˜“æ˜ç»†ï¼‰ â†’ ç»Ÿä¸€æ¨¡æ¿å­—æ®µ
    é€‚é…åˆ—ï¼šå®¢æˆ·åç§°ã€è´¦å·ã€äº¤æ˜“æ—¥æœŸã€äº¤æ˜“æ—¶é—´ã€äº¤æ˜“å¡å·ã€æ‘˜è¦ã€å€Ÿè´·æ–¹å‘ã€äº¤æ˜“é‡‘é¢ã€è´¦æˆ·ä½™é¢ã€
          æŸœå‘˜å·ã€äº¤æ˜“æœºæ„å·ã€äº¤æ˜“æœºæ„åç§°ã€å¯¹æ–¹è´¦å·ã€å¯¹æ–¹æˆ·åã€å¯¹æ–¹è¡Œåã€äº¤æ˜“æµæ°´å·ã€äº¤æ˜“æ¸ é“ã€
          è‡ªåŠ©è®¾å¤‡ç¼–å·ã€æ‰©å……å¤‡æ³¨ã€IPåœ°å€ã€MACåœ°å€ã€ç¬¬ä¸‰æ–¹è®¢å•å·ã€å•†æˆ·å·ã€å•†æˆ·åç§°
    """
    try:
        xls = pd.ExcelFile(p)
        if "äº¤æ˜“æ˜ç»†" not in xls.sheet_names:
            return pd.DataFrame(columns=TEMPLATE_COLS)
        df = xls.parse("äº¤æ˜“æ˜ç»†", header=0)
    except Exception:
        return pd.DataFrame(columns=TEMPLATE_COLS)

    if df.empty:
        return pd.DataFrame(columns=TEMPLATE_COLS)

    df.columns = pd.Index(df.columns).astype(str).str.strip()
    out = pd.DataFrame(index=df.index)

    # åŸºæœ¬å­—æ®µ
    out["æœ¬æ–¹è´¦å·"] = df.get("è´¦å·", "")
    out["æœ¬æ–¹å¡å·"] = df.get("äº¤æ˜“å¡å·", "").astype(str).str.replace(r"\.0$", "", regex=True)
    out["æŸ¥è¯¢è´¦æˆ·"] = out["æœ¬æ–¹è´¦å·"]
    out["æŸ¥è¯¢å¡å·"] = out["æœ¬æ–¹å¡å·"]

    out["æŸ¥è¯¢å¯¹è±¡"] = df.get("å®¢æˆ·åç§°", "").astype(str).replace({"nan":""}).replace("", "æœªçŸ¥")
    out["åé¦ˆå•ä½"] = "å»ºè®¾é“¶è¡Œ"
    out["å¸ç§"] = df.get("å¸ç§", "CNY").astype(str).replace({"äººæ°‘å¸":"CNY","äººæ°‘å¸å…ƒ":"CNY","RMB":"CNY","156":"CNY"}).fillna("CNY")

    amt = pd.to_numeric(df.get("äº¤æ˜“é‡‘é¢", 0), errors="coerce")
    out["äº¤æ˜“é‡‘é¢"] = amt
    out["è´¦æˆ·ä½™é¢"] = pd.to_numeric(df.get("è´¦æˆ·ä½™é¢", ""), errors="coerce")

    # å€Ÿè´·æ–¹å‘ï¼šå€Ÿ->å‡ºï¼Œè´·->è¿›
    jd = df.get("å€Ÿè´·æ–¹å‘", "").astype(str).str.strip()
    out["å€Ÿè´·æ ‡å¿—"] = np.where(jd.str.contains("^è´·", na=False) | jd.str.upper().isin(["è´·","C","CR","CREDIT"]), "è¿›",
                        np.where(jd.str.contains("^å€Ÿ", na=False) | jd.str.upper().isin(["å€Ÿ","D","DR","DEBIT"]), "å‡º",
                                 np.where(amt>0, "è¿›", np.where(amt<0, "å‡º", ""))))

    # æ—¶é—´
    dates = df.get("äº¤æ˜“æ—¥æœŸ", "")
    times = df.get("äº¤æ˜“æ—¶é—´", "")
    times_str = pd.Series(times).astype(str).str.replace(r"\.0$", "", regex=True)
    out["äº¤æ˜“æ—¶é—´"] = [_parse_dt(d, t, is_old=False) for d, t in zip(dates, times_str)]

    # å…¶å®ƒæ˜ å°„
    out["äº¤æ˜“æ‘˜è¦"] = df.get("æ‘˜è¦", " ").astype(str)
    out["äº¤æ˜“ç±»å‹"] = ""  # ä¿ç•™ç©ºä½ï¼ˆå¦‚éœ€ç”±æ‘˜è¦/æ¸ é“äºŒæ¬¡æ¨æ–­å¯è‡ªè¡Œæ‰©å±•ï¼‰
    out["äº¤æ˜“æµæ°´å·"] = df.get("äº¤æ˜“æµæ°´å·", "").astype(str)

    out["äº¤æ˜“å¯¹æ–¹å§“å"] = df.get("å¯¹æ–¹æˆ·å", " ").astype(str)
    out["äº¤æ˜“å¯¹æ–¹è´¦æˆ·"] = df.get("å¯¹æ–¹è´¦å·", " ").astype(str)
    out["äº¤æ˜“å¯¹æ–¹å¡å·"] = ""
    out["äº¤æ˜“å¯¹æ–¹è¯ä»¶å·ç "] = " "
    out["äº¤æ˜“å¯¹æ‰‹ä½™é¢"] = ""
    out["äº¤æ˜“å¯¹æ–¹è´¦å·å¼€æˆ·è¡Œ"] = df.get("å¯¹æ–¹è¡Œå", " ").astype(str)

    out["äº¤æ˜“ç½‘ç‚¹åç§°"] = df.get("äº¤æ˜“æœºæ„åç§°", "").astype(str)
    out["äº¤æ˜“ç½‘ç‚¹ä»£ç "] = df.get("äº¤æ˜“æœºæ„å·", "").astype(str)
    out["äº¤æ˜“æŸœå‘˜å·"] = df.get("æŸœå‘˜å·", "").astype(str)

    out["ç»ˆç«¯å·"] = df.get("äº¤æ˜“æ¸ é“", "").astype(str)  # å¸¸è§å½¢æ€ï¼šæ¸ é“ä»£ç 
    # å…¶å®ƒå¯ç”¨è¡¥å……ä¿¡æ¯ â†’ å¤‡æ³¨
    ext = df.get("æ‰©å……å¤‡æ³¨", "").astype(str).replace({"nan":""})
    out["å¤‡æ³¨"] = ext

    out["ç°é‡‘æ ‡å¿—"] = ""
    out["æ—¥å¿—å·"] = ""
    out["ä¼ ç¥¨å·"] = ""
    out["å‡­è¯ç§ç±»"] = ""
    out["å‡­è¯å·"] = ""

    out["äº¤æ˜“æ˜¯å¦æˆåŠŸ"] = ""
    out["äº¤æ˜“å‘ç”Ÿåœ°"] = ""

    out["å•†æˆ·åç§°"] = df.get("å•†æˆ·åç§°", "").astype(str)
    out["å•†æˆ·å·"] = df.get("å•†æˆ·å·", "").astype(str)
    out["IPåœ°å€"] = df.get("IPåœ°å€", "").astype(str)
    out["MAC"] = df.get("MACåœ°å€", "").astype(str)

    # å¯¹é½æ¨¡æ¿
    out = out.reindex(columns=TEMPLATE_COLS, fill_value="")
    return out

# ------------------------------------------------------------------
# â‘¤.5  é€šè®¯å½•è¯»å–ä¸èŒåŠ¡åŒ¹é…ï¼ˆæ–°å¢ï¼‰
# ------------------------------------------------------------------
CONTACT_NAME_COLS = ["å§“å", "è”ç³»äºº", "äººå‘˜å§“å", "å§“å/åç§°"]
DEPT_COLS = ["ä¸»ç®¡éƒ¨é—¨åç§°", "éƒ¨é—¨", "æ‰€å±éƒ¨é—¨", "å½’å±å•ä½", "å•ä½", "å·¥ä½œå•ä½", "ç§‘å®¤", "å¤„å®¤", "æ‰€å±å•ä½"]
TITLE_COLS = ["è¡Œæ”¿èŒåŠ¡", "å²—ä½", "èŒç§°"]

def _find_first_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    for c in df.columns:
        cs = str(c).strip()
        for key in candidates:
            if key in cs:
                return c
    return None

def _compose_title_str(dept: str, title: str) -> str:
    """èŒåŠ¡æ‹¼æ¥è§„åˆ™ï¼šéƒ¨é—¨-è¡Œæ”¿èŒåŠ¡ï¼›ç¼ºä¸€å–ä¸€ï¼›éƒ½ç©ºåˆ™ç©º"""
    def _blank(x: Any) -> bool:
        s = str(x).strip() if x is not None else ""
        return s == "" or s.lower() in {"nan", "none"} or s in {"-", "â€”", "â€”â€”", "æ— ", "æš‚æ— "}
    d = "" if _blank(dept) else str(dept).strip()
    t = "" if _blank(title) else str(title).strip()

    if d and t:
        return f"{d}-{t}"
    if t:
        return t
    if d:
        return d
    return ""

def _read_one_contacts_sheet(xls: pd.ExcelFile, sheet_name: str) -> pd.DataFrame:
    # å…ˆå°è¯•ç›´æ¥è¯»å¤´ï¼Œå†é€€å›æ‰«æå‰10è¡Œæ‰¾â€œå§“åâ€
    try:
        df0 = xls.parse(sheet_name=sheet_name, header=0)
        df0.columns = pd.Index(df0.columns).astype(str).str.strip()
        if any("å§“å" in c for c in df0.columns):
            df = df0
        else:
            raise ValueError("æœªå‘½ä¸­å§“ååˆ—ï¼Œå°è¯•æ‰«æè¡¨å¤´")
    except Exception:
        head = xls.parse(sheet_name=sheet_name, header=None, nrows=10)
        header_idx = 0
        for i, row in head.iterrows():
            if row.astype(str).str.contains("å§“å").any():
                header_idx = i
                break
        df = xls.parse(sheet_name=sheet_name, header=header_idx)
        df.columns = pd.Index(df.columns).astype(str).str.strip()

    name_col  = _find_first_col(df, CONTACT_NAME_COLS)
    dept_col  = _find_first_col(df, DEPT_COLS)
    title_col = _find_first_col(df, TITLE_COLS)
    if not name_col:
        return pd.DataFrame(columns=["å§“å","èŒåŠ¡"])

    out = pd.DataFrame()
    out["å§“å"] = df[name_col].astype(str).str.strip()
    dept = df[dept_col].astype(str) if dept_col in df else ""
    titl = df[title_col].astype(str) if title_col in df else ""
    if isinstance(dept, str): dept = pd.Series([dept]*len(out))
    if isinstance(titl, str): titl = pd.Series([titl]*len(out))
    out["èŒåŠ¡"] = [_compose_title_str(d, t) for d, t in zip(dept, titl)]
    out = out[(out["å§“å"]!="") & (~out["å§“å"].str.lower().eq("nan"))]
    out["èŒåŠ¡"] = out["èŒåŠ¡"].replace({"nan":"","None":""})
    out.drop_duplicates(inplace=True)
    return out[["å§“å","èŒåŠ¡"]]

def load_contacts_map(root: Path) -> Dict[str, str]:
    files = [p for p in root.rglob("*é€šè®¯å½•*.xls*")]
    if not files:
        print("â„¹ï¸ æœªå‘ç°æ–‡ä»¶ååŒ…å«â€œé€šè®¯å½•â€çš„Excelã€‚")
        return {}

    frames: List[pd.DataFrame] = []
    for p in files:
        try:
            xls = pd.ExcelFile(p)
        except Exception as e:
            print("âŒ é€šè®¯å½•è½½å…¥å¤±è´¥", p.name, e)
            continue
        for sht in xls.sheet_names:
            try:
                df = _read_one_contacts_sheet(xls, sht)
                if not df.empty:
                    df["æ¥æºæ–‡ä»¶"] = p.name
                    df["æ¥æºsheet"] = sht
                    frames.append(df)
            except Exception as e:
                print("âŒ é€šè®¯å½•è§£æå¤±è´¥", f"{p.name}->{sht}", e)

    if not frames:
        print("â„¹ï¸ é€šè®¯å½•ä¸­æœªè§£æå‡ºæœ‰æ•ˆå§“å/èŒåŠ¡ã€‚")
        return {}

    allc = pd.concat(frames, ignore_index=True)
    allc["å§“å"] = allc["å§“å"].astype(str).str.strip()
    allc["èŒåŠ¡"] = allc["èŒåŠ¡"].astype(str).str.strip()

    def _uniq_preserve(seq: List[str]) -> List[str]:
        """å»ç©ºå»'nan'å¹¶æŒ‰å‡ºç°é¡ºåºå»é‡"""
        seen = set()
        out: List[str] = []
        for x in seq:
            s = (x or "").strip()
            if not s or s.lower() == "nan":
                continue
            if s not in seen:
                seen.add(s)
                out.append(s)
        return out

    # åŒåå¤šæ¡ -> åˆå¹¶æˆ 'ã€' åˆ†éš”çš„ä¸€æ¡
    grouped = (allc.groupby("å§“å")["èŒåŠ¡"]
                    .apply(lambda s: "ã€".join(_uniq_preserve(list(s)))))
    mapping = grouped.to_dict()

    print(f"âœ… å·²è¯»å–é€šè®¯å½• {len(files)} ä»½ï¼Œæ”¶å½•å§“å {len(mapping)} æ¡ï¼ˆåŒåå·²åˆå¹¶ï¼‰ã€‚")
    return mapping

# ------------------------------------------------------------------
# â‘¥   åˆå¹¶å…¨éƒ¨æµæ°´
# ------------------------------------------------------------------
def merge_all_txn(root_dir: str) -> pd.DataFrame:
    root = Path(root_dir).expanduser().resolve()

    # â€”â€” æ–°å¢ï¼šå…ˆåŠ è½½é€šè®¯å½•ï¼ˆå…¨å±€æ˜ å°„ï¼‰
    global CONTACT_TITLE_MAP
    CONTACT_TITLE_MAP = load_contacts_map(root)

    china_files = [p for p in root.rglob("*-*-äº¤æ˜“æµæ°´.xls*")]

    all_excel = list(root.rglob("*.xls*"))
    rc_candidates = [p for p in all_excel if "å†œå•†è¡Œ" in p.as_posix()]
    pattern_old = re.compile(r"è€\s*[è´¦å¸]\s*(?:å·|æˆ·)")
    old_rc = [p for p in rc_candidates if pattern_old.search(p.stem)]
    new_rc = [p for p in rc_candidates if p not in old_rc]

    tl_files = [p for p in all_excel if "æ³°éš†" in p.as_posix()]
    mt_files = [p for p in all_excel if "æ°‘æ³°" in p.as_posix()]

    # â€”â€” æ–°å¢ï¼šå†œè¡Œçº¿ä¸‹ï¼ˆAPSHï¼‰ã€å»ºè¡Œçº¿ä¸‹ï¼ˆäº¤æ˜“æ˜ç»†ï¼‰
    abc_offline_files = [p for p in all_excel if _is_abc_offline_file(p)]
    ccb_offline_files = [p for p in all_excel if _is_ccb_offline_file(p)]

    csv_txn_files = [p for p in root.rglob("äº¤æ˜“æ˜ç»†ä¿¡æ¯.csv")]

    print(
        f"âœ… ç½‘ä¸Šé“¶è¡Œ {len(china_files)} ä»½ï¼Œ"
        f"è€å†œå•† {len(old_rc)} ä»½ï¼Œæ–°å†œå•† {len(new_rc)} ä»½ï¼Œ"
        f"æ³°éš†é“¶è¡Œ {len(tl_files)} ä»½ï¼Œ"
        f"æ°‘æ³°é“¶è¡Œ {len(mt_files)} ä»½ï¼Œ"
        f"å†œè¡Œçº¿ä¸‹ {len(abc_offline_files)} ä»½ï¼Œ"
        f"å»ºè¡Œçº¿ä¸‹ {len(ccb_offline_files)} ä»½ï¼Œ"
        f"äº¤æ˜“æ˜ç»†CSV {len(csv_txn_files)} ä»½"
    )

    dfs: List[pd.DataFrame] = []

    # â€”â€” ç½‘é“¶æ ‡å‡†è¡¨ â€”â€” ç›´æ¥è¯»å…¥
    for p in china_files:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        try:
            df = pd.read_excel(
                p,
                dtype={
                    "æŸ¥è¯¢å¡å·": str,
                    "æŸ¥è¯¢è´¦æˆ·": str,
                    "äº¤æ˜“å¯¹æ–¹è¯ä»¶å·ç ": str,
                    "æœ¬æ–¹è´¦å·": str,
                    "æœ¬æ–¹å¡å·": str,
                },
            )
            df["æ¥æºæ–‡ä»¶"] = p.name
            dfs.append(df)
        except Exception as e:
            print("âŒ", p.name, e)

    # â€”â€” å†œå•†è¡Œ â€”â€” æ–°æ—§åˆ†æµ
    for p in old_rc + new_rc:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        kw = should_skip_special(p)
        if kw:
            print(f"â© è·³è¿‡ã€{p.name}ã€‘ï¼šè¡¨å¤´å«â€œ{kw}â€")
            continue

        raw = _read_raw(p)

        holder = extract_holder_from_df(raw) or holder_from_folder(p.parent) or fallback_holder_from_path(p)
        is_old = p in old_rc
        df = rc_to_template(raw, holder, is_old)
        if not df.empty:
            df["æ¥æºæ–‡ä»¶"] = p.name
            dfs.append(df)

    # â€”â€” æ³°éš† â€”â€” 
    for p in tl_files:
        if "å¼€æˆ·" in p.stem:
            continue
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        try:
            xls = pd.ExcelFile(p)
        except Exception as e:
            print("âŒ", f"{p.name} è½½å…¥å¤±è´¥", e)
            continue

        try:
            header_idx = _header_row(p)  # ç¼“å­˜åä»…è®¡ç®—ä¸€æ¬¡
        except Exception as e:
            print("âŒ", f"{p.name} è¡¨å¤´è¡Œè¯†åˆ«å¤±è´¥", e)
            header_idx = 0

        xls_dict: Dict[str, pd.DataFrame] = {}
        for sht in xls.sheet_names:
            try:
                df_sheet = xls.parse(sheet_name=sht, header=header_idx)
                xls_dict[sht] = df_sheet
            except Exception as e:
                print("âŒ", f"{p.name} -> {sht}", e)

        df = tl_to_template(xls_dict)
        if not df.empty:
            df["æ¥æºæ–‡ä»¶"] = p.name
            dfs.append(df)

    # â€”â€” æ°‘æ³° â€”â€” å¸¸è§„
    for p in mt_files:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        raw = _read_raw(p)
        df  = mt_to_template(raw)
        if not df.empty:
            df["æ¥æºæ–‡ä»¶"] = p.name
            dfs.append(df)

    # â€”â€” å†œè¡Œçº¿ä¸‹ â€”â€” APSH
    for p in abc_offline_files:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        try:
            df = abc_offline_from_file(p)
            if not df.empty:
                df["æ¥æºæ–‡ä»¶"] = p.name
                dfs.append(df)
        except Exception as e:
            print("âŒ å†œè¡Œçº¿ä¸‹è§£æå¤±è´¥", p.name, e)

    # â€”â€” å»ºè¡Œçº¿ä¸‹ â€”â€” äº¤æ˜“æ˜ç»†
    for p in ccb_offline_files:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        try:
            df = ccb_offline_from_file(p)
            if not df.empty:
                df["æ¥æºæ–‡ä»¶"] = p.name
                dfs.append(df)
        except Exception as e:
            print("âŒ å»ºè¡Œçº¿ä¸‹è§£æå¤±è´¥", p.name, e)

    # â€”â€” äº¤æ˜“æ˜ç»† CSV
    for p in csv_txn_files:
        print(f"æ­£åœ¨å¤„ç† {p.name} ...")
        try:
            raw_csv = _read_csv_smart(p)
        except Exception as e:
            print("âŒ æ— æ³•è¯»å–CSV", p.name, e)
            continue

        holder = _person_from_people_csv(p.parent) or holder_from_folder(p.parent) or fallback_holder_from_path(p)
        feedback_unit = p.parent.name
        try:
            df = csv_to_template(raw_csv, holder, feedback_unit)
        except Exception as e:
            print("âŒ CSVè½¬æ¨¡æ¿å¤±è´¥", p.name, e)
            continue

        if not df.empty:
            df["æ¥æºæ–‡ä»¶"] = p.name
            dfs.append(df)

    print(f"æ–‡ä»¶è¯»å–å·²å®Œæˆï¼Œæ­£åœ¨æ•´åˆåˆ†æï¼ ...")

    if not dfs:
        return pd.DataFrame(columns=TEMPLATE_COLS)

    all_txn = pd.concat(dfs, ignore_index=True)

    # â€”â€” æ–°å¢ï¼šä¸‰é”®å»é‡ï¼ˆäº¤æ˜“æµæ°´å· + äº¤æ˜“æ—¶é—´ + äº¤æ˜“é‡‘é¢ï¼‰
    # è¯´æ˜ï¼šä¸ºé¿å…å› é‡‘é¢æ ¼å¼å·®å¼‚å¯¼è‡´çš„â€œå‡ä¸åŒâ€ï¼Œå°†é‡‘é¢å…ˆè½¬ä¸ºæ•°å€¼å¹¶ä¿ç•™ä¸¤ä½å°æ•°å†å»é‡
    all_txn["äº¤æ˜“é‡‘é¢"] = pd.to_numeric(all_txn["äº¤æ˜“é‡‘é¢"], errors="coerce").round(2)
    before = len(all_txn)
    all_txn = all_txn.drop_duplicates(subset=["äº¤æ˜“æµæ°´å·", "äº¤æ˜“æ—¶é—´", "äº¤æ˜“é‡‘é¢"], keep="first").reset_index(drop=True)
    removed = before - len(all_txn)
    if removed:
        print(f"ğŸ§¹ å·²æŒ‰â€œäº¤æ˜“æµæ°´å·+äº¤æ˜“æ—¶é—´+äº¤æ˜“é‡‘é¢â€å»é‡ {removed} æ¡ã€‚")

    # â€”â€” ç»Ÿä¸€ï¼šæ’åºã€åºå·ã€ç±»å‹æ ‡å‡†åŒ–ã€åˆ†ç®±ã€æ˜ŸæœŸ/èŠ‚å‡æ—¥ â€”â€” å‘é‡åŒ–åŠ é€Ÿ
    ts = pd.to_datetime(all_txn["äº¤æ˜“æ—¶é—´"], errors="coerce")
    all_txn.insert(0, "__ts__", ts)
    all_txn.sort_values("__ts__", inplace=True, kind="mergesort")  # ç¨³å®šæ’åº
    all_txn["åºå·"] = range(1, len(all_txn) + 1)
    all_txn.drop(columns="__ts__", inplace=True)

    all_txn["äº¤æ˜“é‡‘é¢"] = pd.to_numeric(all_txn["äº¤æ˜“é‡‘é¢"], errors="coerce")

    # å€Ÿè´·æ ‡å¿—æ ‡å‡†åŒ–
    def _std_flag(x):
        if pd.isna(x):
            return x
        s = str(x).strip()
        if s in {"1","å€Ÿ","D"}: return "å‡º"
        if s in {"2","è´·","C"}: return "è¿›"
        return s
    all_txn["å€Ÿè´·æ ‡å¿—"] = all_txn["å€Ÿè´·æ ‡å¿—"].apply(_std_flag)

    # é‡‘é¢åˆ†ç®±
    bins = [-np.inf, 2000, 5000, 20000, 50000, np.inf]
    labels = ["2000ä»¥ä¸‹","2000-5000","5000-20000","20000-50000","50000ä»¥ä¸Š"]
    all_txn["é‡‘é¢åŒºé—´"] = pd.cut(all_txn["äº¤æ˜“é‡‘é¢"], bins=bins, labels=labels, right=False, include_lowest=True)

    # æ˜ŸæœŸï¼ˆå‘é‡åŒ–ï¼‰
    weekday_map = {0:"æ˜ŸæœŸä¸€",1:"æ˜ŸæœŸäºŒ",2:"æ˜ŸæœŸä¸‰",3:"æ˜ŸæœŸå››",4:"æ˜ŸæœŸäº”",5:"æ˜ŸæœŸå…­",6:"æ˜ŸæœŸæ—¥"}
    wk = pd.Series(index=all_txn.index, dtype=object)
    mask_valid = ts.notna()
    wk.loc[mask_valid] = ts.dt.weekday.map(weekday_map)
    wk.loc[~mask_valid] = "wrong"
    all_txn["æ˜ŸæœŸ"] = wk

    # èŠ‚å‡æ—¥ï¼ˆå¯¹å”¯ä¸€æ—¥æœŸåšç¼“å­˜æ˜ å°„ï¼‰
    dates = ts.dt.date
    status = pd.Series(index=all_txn.index, dtype=object)
    unique_dates = pd.unique(dates[mask_valid])
    @lru_cache(maxsize=None)
    def _day_status(d) -> str:
        try:
            return "èŠ‚å‡æ—¥" if is_holiday(d) else ("å·¥ä½œæ—¥" if is_workday(d) else "å‘¨æœ«")
        except Exception:
            # é«˜ç¨³å¥å…œåº•
            dt = datetime.datetime.combine(d, datetime.time())
            return "å‘¨æœ«" if dt.weekday() >= 5 else "å·¥ä½œæ—¥"
    if len(unique_dates):
        map_dict = {d: _day_status(d) for d in unique_dates}
        status.loc[mask_valid] = dates.loc[mask_valid].map(map_dict)
    status.loc[~mask_valid] = "wrong"
    all_txn["èŠ‚å‡æ—¥"] = status

    # â€”â€” æ–°å¢ï¼šåŒ¹é…é€šè®¯å½•ï¼Œå¢åŠ â€œå¯¹æ–¹èŒåŠ¡â€åˆ—
    if CONTACT_TITLE_MAP:
        all_txn["å¯¹æ–¹èŒåŠ¡"] = all_txn["äº¤æ˜“å¯¹æ–¹å§“å"].map(CONTACT_TITLE_MAP).fillna("")
    else:
        all_txn["å¯¹æ–¹èŒåŠ¡"] = ""

    save_df_auto_width(all_txn, "æ‰€æœ‰äºº-åˆå¹¶äº¤æ˜“æµæ°´", index=False, engine="openpyxl")
    print("âœ… å·²å¯¼å‡º æ‰€æœ‰äºº-åˆå¹¶äº¤æ˜“æµæ°´.xlsx")
    return all_txn

# ------------------------------------------------------------------
# â‘¦   å•äººèµ„äº§ / å¯¹æ‰‹åˆ†æ
# ------------------------------------------------------------------
def analysis_txn(df: pd.DataFrame) -> None:
    if df.empty:
        return
    df = df.copy()
    df["äº¤æ˜“æ—¶é—´"] = pd.to_datetime(df["äº¤æ˜“æ—¶é—´"], errors="coerce")
    df["äº¤æ˜“é‡‘é¢"] = pd.to_numeric(df["äº¤æ˜“é‡‘é¢"], errors="coerce")
    person = df["æŸ¥è¯¢å¯¹è±¡"].iat[0] or "æœªçŸ¥"
    prefix = f"{person}/"

    out_df = df[df["å€Ÿè´·æ ‡å¿—"] == "å‡º"]
    in_df = df[df["å€Ÿè´·æ ‡å¿—"] == "è¿›"]
    counts = df["é‡‘é¢åŒºé—´"].value_counts()

    summary = pd.DataFrame([{
        "äº¤æ˜“æ¬¡æ•°": len(df),
        "äº¤æ˜“é‡‘é¢": df["äº¤æ˜“é‡‘é¢"].sum(skipna=True),
        "æµå‡ºé¢": out_df["äº¤æ˜“é‡‘é¢"].sum(skipna=True),
        "æµå…¥é¢": in_df["äº¤æ˜“é‡‘é¢"].sum(skipna=True),
        "å•ç¬”æœ€å¤§æ”¯å‡º": out_df["äº¤æ˜“é‡‘é¢"].max(skipna=True),
        "å•ç¬”æœ€å¤§æ”¶å…¥": in_df["äº¤æ˜“é‡‘é¢"].max(skipna=True),
        "å‡€æµå…¥": in_df["äº¤æ˜“é‡‘é¢"].sum(skipna=True) - out_df["äº¤æ˜“é‡‘é¢"].sum(skipna=True),
        "æœ€åäº¤æ˜“æ—¶é—´": df["äº¤æ˜“æ—¶é—´"].max(),
        "0-2åƒæ¬¡æ•°": counts.get("2000ä»¥ä¸‹", 0),
        "2åƒ-5åƒæ¬¡æ•°": counts.get("2000-5000", 0),
        "5åƒ-2ä¸‡æ¬¡æ•°": counts.get("5000-20000", 0),
        "2ä¸‡-5ä¸‡æ¬¡æ•°": counts.get("20000-50000", 0),
        "5ä¸‡ä»¥ä¸Šæ¬¡æ•°": counts.get("50000ä»¥ä¸Š", 0),
    }])
    save_df_auto_width(summary, f"{prefix}0{person}-èµ„äº§åˆ†æ", index=False, engine="openpyxl")

    cash = df[(df["ç°é‡‘æ ‡å¿—"].astype(str).str.contains("ç°", na=False) 
               | (pd.to_numeric(df["ç°é‡‘æ ‡å¿—"], errors="coerce") == 1) 
               | df["äº¤æ˜“ç±»å‹"].astype(str).str.contains("æŸœé¢|ç°", na=False))
                & (pd.to_numeric(df["äº¤æ˜“é‡‘é¢"], errors="coerce") >= 10_000)]
    save_df_auto_width(cash, f"{prefix}1{person}-å­˜å–ç°1ä¸‡ä»¥ä¸Š", index=False, engine="openpyxl")

    big = df[pd.to_numeric(df["äº¤æ˜“é‡‘é¢"], errors="coerce") >= 500_000]
    save_df_auto_width(big, f"{prefix}1{person}-å¤§é¢èµ„é‡‘50ä¸‡ä»¥ä¸Š", index=False, engine="openpyxl")

    src = df.copy()
    src["is_in"] = src["å€Ÿè´·æ ‡å¿—"] == "è¿›"
    src["signed_amt"] = pd.to_numeric(src["äº¤æ˜“é‡‘é¢"], errors="coerce") * src["is_in"].map({True: 1, False: -1})
    src["in_amt"] = pd.to_numeric(src["äº¤æ˜“é‡‘é¢"], errors="coerce").where(src["is_in"], 0)
    src = (src.groupby("äº¤æ˜“å¯¹æ–¹å§“å", dropna=False)
           .agg(äº¤æ˜“é‡‘é¢=("äº¤æ˜“é‡‘é¢","sum"),
                äº¤æ˜“æ¬¡æ•°=("äº¤æ˜“é‡‘é¢","size"),
                æµå…¥é¢=("in_amt","sum"),
                å‡€æµå…¥=("signed_amt","sum"),
                å•ç¬”æœ€å¤§æ”¶å…¥=("in_amt","max"))
           .reset_index())
    total = src["æµå…¥é¢"].sum()
    src["æµå…¥æ¯”%"] = src["æµå…¥é¢"] / total * 100 if total else 0
    # æ–°å¢ï¼šå¯¹æ–¹èŒåŠ¡ï¼ˆæ¥è‡ªé€šè®¯å½•æ˜ å°„ï¼‰
    if CONTACT_TITLE_MAP:
        src.insert(1, "å¯¹æ–¹èŒåŠ¡", src["äº¤æ˜“å¯¹æ–¹å§“å"].map(CONTACT_TITLE_MAP).fillna(""))

    src.sort_values("æµå…¥é¢", ascending=False, inplace=True)
    save_df_auto_width(src, f"{prefix}1{person}-èµ„é‡‘æ¥æºåˆ†æ", index=False, engine="openpyxl")

def make_partner_summary(df: pd.DataFrame) -> None:
    if df.empty:
        return
    person = df["æŸ¥è¯¢å¯¹è±¡"].iat[0] or "æœªçŸ¥"
    prefix = f"{person}/"
    d = df.copy()
    d["äº¤æ˜“é‡‘é¢"] = pd.to_numeric(d["äº¤æ˜“é‡‘é¢"], errors="coerce").fillna(0)
    d["is_in"] = d["å€Ÿè´·æ ‡å¿—"] == "è¿›"
    d["abs_amt"] = d["äº¤æ˜“é‡‘é¢"].abs()
    d["signed_amt"] = d["äº¤æ˜“é‡‘é¢"] * d["is_in"].map({True: 1, False: -1})
    d["in_amt"] = d["äº¤æ˜“é‡‘é¢"].where(d["is_in"], 0)
    d["out_amt"] = d["äº¤æ˜“é‡‘é¢"].where(~d["is_in"], 0)
    d["gt10k"] = (d["abs_amt"] >= 10_000).astype(int)
    summ = (d.groupby(["æŸ¥è¯¢å¯¹è±¡","äº¤æ˜“å¯¹æ–¹å§“å"], dropna=False)
              .agg(äº¤æ˜“æ¬¡æ•°=("äº¤æ˜“é‡‘é¢","size"),
                   äº¤æ˜“é‡‘é¢=("abs_amt","sum"),
                   ä¸‡å…ƒä»¥ä¸Šäº¤æ˜“æ¬¡æ•°=("gt10k","sum"),
                   å‡€æ”¶å…¥=("signed_amt","sum"),
                   è½¬å…¥ç¬”æ•°=("is_in","sum"),
                   è½¬å…¥é‡‘é¢=("in_amt","sum"),
                   è½¬å‡ºç¬”æ•°=("is_in", lambda x: (~x).sum()),
                   è½¬å‡ºé‡‘é¢=("out_amt","sum"))
              .reset_index()
              .rename(columns={"æŸ¥è¯¢å¯¹è±¡":"å§“å","äº¤æ˜“å¯¹æ–¹å§“å":"å¯¹æ–¹å§“å"}))

    # æ–°å¢ï¼šå¯¹æ–¹èŒåŠ¡
    if CONTACT_TITLE_MAP:
        summ.insert(2, "å¯¹æ–¹èŒåŠ¡", summ["å¯¹æ–¹å§“å"].map(CONTACT_TITLE_MAP).fillna(""))

    total = summ.groupby("å§“å")["äº¤æ˜“é‡‘é¢"].transform("sum")
    summ["äº¤æ˜“å æ¯”%"] = np.where(total>0, summ["äº¤æ˜“é‡‘é¢"] / total * 100, 0)
    summ.sort_values(["å§“å","äº¤æ˜“é‡‘é¢"], ascending=[True, False], inplace=True)
    save_df_auto_width(summ, f"{prefix}2{person}-äº¤æ˜“å¯¹æ‰‹åˆ†æ", index=False, engine="openpyxl")

    comp = summ[summ["å¯¹æ–¹å§“å"].astype(str).str.contains("å…¬å¸", na=False)]
    save_df_auto_width(comp, f"{prefix}3{person}-ä¸å…¬å¸ç›¸å…³äº¤æ˜“é¢‘æ¬¡åˆ†æ", index=False, engine="openpyxl")

# ------------------------------------------------------------------
# â‘§   GUI
# ------------------------------------------------------------------
def create_gui():
    root = tk.Tk()
    root.title("æ¸©å²­çºªå§”äº¤æ˜“æµæ°´æ‰¹é‡åˆ†æå·¥å…·")
    root.minsize(780, 560)
    ttk.Label(root, text="æ¸©å²­çºªå§”äº¤æ˜“æµæ°´æ‰¹é‡åˆ†æå·¥å…·", font=("ä»¿å®‹", 20, "bold")).grid(row=0, column=0, columnspan=3, pady=(15, 0))
    ttk.Label(root, text="Â© æ¸©å²­çºªå§”å…­å®¤ å•æŸ³æ˜Š", font=("å¾®è½¯é›…é»‘", 9)).grid(row=1, column=0, columnspan=3, pady=(0, 15))

    ttk.Label(root, text="å·¥ä½œç›®å½•:").grid(row=2, column=0, sticky="e", padx=8, pady=8)
    path_var = tk.StringVar(value=str(Path.home()))
    ttk.Entry(root, textvariable=path_var, width=60).grid(row=2, column=1, sticky="we", padx=5, pady=8)
    ttk.Button(
        root,
        text="æµè§ˆ...",
        command=lambda: path_var.set(filedialog.askdirectory(title="é€‰æ‹©å·¥ä½œç›®å½•") or path_var.get()),
    ).grid(row=2, column=2, padx=5, pady=8)

    log_box = tk.Text(root, width=90, height=18, state="disabled")
    log_box.grid(row=4, column=0, columnspan=3, padx=10, pady=(5, 10), sticky="nsew")
    root.columnconfigure(1, weight=1)
    root.rowconfigure(4, weight=1)

    def log(msg):
        log_box.config(state="normal")
        log_box.insert("end", f"{datetime.datetime.now():%H:%M:%S}  {msg}\n")
        log_box.config(state="disabled")
        log_box.see("end")

    def run(path):
        global OUT_DIR
        OUT_DIR = Path(path).expanduser().resolve() / "æ‰¹é‡åˆ†æç»“æœ"
        OUT_DIR.mkdir(parents=True, exist_ok=True)
        _orig_print = builtins.print
        builtins.print = lambda *a, **k: log(" ".join(map(str, a)))
        try:
            all_txn = merge_all_txn(path)
            if all_txn.empty:
                messagebox.showinfo("å®Œæˆ", "æœªæ‰¾åˆ°å¯åˆ†ææ–‡ä»¶")
                return
            for person, df_person in all_txn.groupby("æŸ¥è¯¢å¯¹è±¡", dropna=False):
                print(f"--- åˆ†æ {person} ---")
                analysis_txn(df_person)
                make_partner_summary(df_person)
            messagebox.showinfo("å®Œæˆ", f"å…¨éƒ¨åˆ†æå®Œæˆï¼ç»“æœåœ¨:\n{OUT_DIR}")
        except Exception as e:
            messagebox.showerror("é”™è¯¯", str(e))
        finally:
            builtins.print = _orig_print

    def on_start():
        p = path_var.get().strip()
        if not p:
            messagebox.showwarning("æç¤º", "è¯·å…ˆé€‰æ‹©å·¥ä½œç›®å½•ï¼")
            return
        threading.Thread(target=run, args=(p,), daemon=True).start()

    ttk.Button(root, text="å¼€å§‹åˆ†æ", command=on_start, width=18).grid(row=3, column=1, pady=10)
    root.mainloop()

# ------------------------------------------------------------------
if __name__ == "__main__":
    create_gui()
